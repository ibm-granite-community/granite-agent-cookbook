{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Observability with Langfuse Tracing\n",
    "\n",
    "This notebook demonstrates how to capture traces of AI agents using **Langfuse** for observability and tracing and how to read a trace.\n",
    "\n",
    "1. **Langfuse Setup** - Installing and configuring Langfuse for tracing\n",
    "2. **Agent Implementation** - Building a simple agent\n",
    "3. **Automatic Trace Capture** - Capture a trace with callback handler.\n",
    "4. **Collect and read a trace** - Collect and read a trace.\n",
    "5. **Manual Trace Capture** - Capture a trace manually and how to read it\n",
    "\n",
    "## Why Agent Observability?\n",
    "\n",
    "Before adding complexity to your agent architecture, validate whether additional components are truly needed.\n",
    "\n",
    "**The process:**\n",
    "- Capture agent traces to understand current behavior\n",
    "- Define a test set covering desired use cases\n",
    "- Run evaluations against the agent\n",
    "- Measure trajectory accuracy and response quality\n",
    "\n",
    "This data-driven approach ensures you only add complexity when justified by actual performance gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langfuse langchain langgraph langchain_ibm pandas python-dotenv matplotlib seaborn rich\n",
    "!pip install -q \"git+https://github.com/ibm-granite-community/utils.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Optional\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "from typing import Any\n",
    "from typing import Annotated, TypedDict\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain_core.messages import AnyMessage, AIMessage, HumanMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langfuse import Langfuse\n",
    "from langfuse import Evaluation\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langfuse import get_client\n",
    "from langfuse.api.resources.commons.types.trace_with_full_details import TraceWithFullDetails\n",
    "from rich import print\n",
    "\n",
    "model = \"ibm/granite-4-h-small\"\n",
    "\n",
    "model_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langfuse Setup\n",
    "\n",
    "Langfuse can be deployed either locally (self-hosted) or on the cloud:\n",
    "\n",
    "**Self-hosted (local):**\n",
    "\n",
    "Follow the Docker Compose deployment [guide](https://langfuse.com/self-hosting/deployment/docker-compose)\n",
    "\n",
    "**Cloud:**\n",
    "\n",
    "Sign up for [cloud account](https://us.cloud.langfuse.com/)\n",
    "\n",
    "### Configuration\n",
    "\n",
    "1. Create a new project\n",
    "2. Navigate to Settings → API Keys\n",
    "3. Create new API keys and copy:\n",
    "   * Public Key\n",
    "   * Secret Key\n",
    "   * Host URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure watsonx.ai\n",
    "\n",
    "You'll need the following environment variables for watsonx.ai:\n",
    "- `WATSONX_URL`: Your watsonx.ai endpoint URL\n",
    "- `WATSONX_APIKEY`: Your watsonx.ai API key\n",
    "- `WATSONX_PROJECT_ID`: Your watsonx.ai project ID\n",
    "\n",
    "Set these in a `.env` file or export them as environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Langfuse Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGFUSE_PUBLIC_KEY=convert_to_secret_str(get_env_var(\"LANGFUSE_PUBLIC_KEY\", \"unset\"))\n",
    "LANGFUSE_SECRET_KEY=convert_to_secret_str(get_env_var(\"LANGFUSE_SECRET_KEY\", \"unset\"))\n",
    "LANGFUSE_HOST=convert_to_secret_str(get_env_var(\"LANGFUSE_HOST\", \"unset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_client = Langfuse(\n",
    "    public_key=os.environ['LANGFUSE_PUBLIC_KEY'],\n",
    "    secret_key=os.environ['LANGFUSE_SECRET_KEY'],\n",
    "    host=os.environ['LANGFUSE_HOST']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent Implementation\n",
    "\n",
    "### Build a Minimal Function Calling Agent\n",
    "\n",
    "Create the FC agent using LangChain's `create_agent` from [Function_Calling](https://github.com/ibm-granite-community/granite-agent-cookbook/blob/main/recipes/Function_Calling/Function_Calling_Agent.ipynb) recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_STOCK_API_KEY = convert_to_secret_str(get_env_var(\"AV_STOCK_API_KEY\", \"unset\"))\n",
    "\n",
    "WEATHER_API_KEY = convert_to_secret_str(get_env_var(\"WEATHER_API_KEY\", \"unset\"))\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=model_parameters,\n",
    ")\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def get_stock_price(ticker: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the lowest and highest stock prices for a given ticker and date.\n",
    "\n",
    "    Args:\n",
    "        ticker: The stock ticker symbol, for example, \"IBM\".\n",
    "        date: The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the low and high stock prices on the given date.\n",
    "    \"\"\"\n",
    "    print(f\"Getting stock price for {ticker} on {date}\")\n",
    "\n",
    "    apikey = AV_STOCK_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"low\": \"245.4500\",\n",
    "            \"high\": \"249.0300\"\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={apikey}\"\n",
    "        stock_data = requests.get(stock_url)\n",
    "        data = stock_data.json()\n",
    "        stock_low = data[\"Time Series (Daily)\"][date][\"3. low\"]\n",
    "        stock_high = data[\"Time Series (Daily)\"][date][\"2. high\"]\n",
    "        return {\n",
    "            \"low\": stock_low,\n",
    "            \"high\": stock_high\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return {\n",
    "            \"low\": \"$245.45\",\n",
    "            \"high\": \"$249.03\"\n",
    "        }\n",
    "\n",
    "\n",
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location (default: San Francisco).\n",
    "\n",
    "    Args:\n",
    "        location: The name of the city for which to retrieve the weather information.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing weather information such as temperature in celsius, weather description, and humidity.\n",
    "    \"\"\"\n",
    "    print(f\"Getting current weather for {location}\")\n",
    "    apikey=WEATHER_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"description\": \"thunderstorms\",\n",
    "            \"temperature\": 25.3,\n",
    "            \"humidity\": 94\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # API request to fetch weather data\n",
    "        weather_url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={apikey}&units=metric\"\n",
    "        weather_data = requests.get(weather_url)\n",
    "        data = weather_data.json()\n",
    "        # Extracting relevant weather details\n",
    "        weather_description = data[\"weather\"][0][\"description\"]\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "\n",
    "        # Returning weather details\n",
    "        return {\n",
    "            \"description\": weather_description,\n",
    "            \"temperature\": temperature,\n",
    "            \"humidity\": humidity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return {\n",
    "            \"description\": \"none\",\n",
    "            \"temperature\": \"none\",\n",
    "            \"humidity\": \"none\"\n",
    "        }\n",
    "\n",
    "\n",
    "def route_tools(state: State) -> str:\n",
    "    \"\"\"\n",
    "    This is conditional_edge function to route to the ToolNode if the last message\n",
    "    in the state has tool calls. Otherwise, route to the END node to complete the\n",
    "    workflow.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\")\n",
    "    if not messages:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    # If the last message is from the model and it contains a tool call request\n",
    "    if isinstance(last_message, AIMessage) and len(last_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"Create and return the LLM instance.\"\"\"\n",
    "    return init_chat_model(\n",
    "        model=MODEL,\n",
    "        model_provider=\"ibm\",\n",
    "        url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "        apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "        project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "        params=MODEL_PARAMETERS,\n",
    "    )\n",
    "\n",
    "def llm_node(state: State) -> State:\n",
    "    messages = state[\"messages\"]\n",
    "    response_message = llm_with_tools.invoke(messages)\n",
    "    state_update = State(messages=[response_message])\n",
    "    return state_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_stock_price, get_current_weather]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"llm\", llm_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_edge(START, \"llm\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    route_tools,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "\n",
    "graph: CompiledStateGraph[State] = graph_builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "def function_calling_agent(graph: CompiledStateGraph, user_input: str):\n",
    "    user_message = HumanMessage(user_input)\n",
    "    print(user_message.pretty_repr())\n",
    "    input = State(messages=[user_message])\n",
    "    for event in graph.stream(input):\n",
    "        for value in event.values():\n",
    "            print(value[\"messages\"][-1].pretty_repr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_calling_agent(graph, \"What is the weather in Miami?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Langfuse experiment to evaluate the agent output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How an experiment Works\n",
    "\n",
    "1. Create dataset items with inputs and expected outputs\n",
    "2. Define your task function to test\n",
    "3. Write evaluators to score results\n",
    "4. Run experiment to get automated scores across all test cases\n",
    "\n",
    "| Component | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **Dataset** | Collection of test cases | defined as `local_data` in this notebook with stock price questions |\n",
    "| **Dataset Item** | Single test case with input and optional expected output | `{\"input\": \"What were...\", \"expected_output\": \"On September...\"}` |\n",
    "| **Task** | Application code being tested | `agent_execution_to_evaluate()` - executes your agent |\n",
    "| **Evaluator** | Function that scores outputs | `accuracy_evaluator()` - checks if expected output appears in response |\n",
    "| **Score** | Evaluation result (numeric/categorical/boolean) | `1.0` (correct) or `0.0` (incorrect) |\n",
    "| **Experiment Run** | Execution of task on all dataset items | `langfuse_client.run_experiment()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_execution_to_evaluate(*, item, **kwargs):\n",
    "    user_message = HumanMessage(item[\"input\"])\n",
    "    response = graph.invoke( {\"messages\": [user_message]})\n",
    "    return response.get('messages')[-1].content\n",
    "    \n",
    "local_data = [\n",
    "    {\n",
    "    \"input\": \"What were the IBM stock prices on September 5, 2025?\", \n",
    "    \"expected_output\": \"On September 5, 2025, the stock price of IBM ranged from a low of $245.45 to a high of $249.03.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Define evaluation functions\n",
    "def accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\n",
    "    if expected_output and expected_output.lower() in output.lower():\n",
    "        return Evaluation(name=\"accuracy\", value=1.0, comment=\"Correct answer found\")\n",
    "\n",
    "    return Evaluation(name=\"accuracy\", value=0.0, comment=\"Incorrect answer\")\n",
    "\n",
    "\n",
    "def length_evaluator(*, input, output, **kwargs):\n",
    "    return Evaluation(name=\"response_length\", value=len(output), comment=f\"Response has {len(output)} characters\")\n",
    "    \n",
    "result = langfuse_client.run_experiment(\n",
    "    name=\"Stock Market Quizz\",\n",
    "    description=\"Testing basic functionality\",\n",
    "    data=local_data,\n",
    "    task=agent_execution_to_evaluate,\n",
    "    evaluators=[accuracy_evaluator, length_evaluator]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the experiment in Langfuse UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiment, you can view the results in the Langfuse dashboard.\n",
    "\n",
    "In the above example our experiment tracked two key metrics:\n",
    "\n",
    "**Accuracy**: Whether the model's response contained the expected stock price information\n",
    "\n",
    "**Response Length**: The character count of each response, helping assess verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![experiment](experiment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Send an Automatic Trace to Langfuse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a trace ?\n",
    "\n",
    "A Langfuse trace represents a single request or operation in your AI application.\n",
    "\n",
    "It captures the entire lifecycle of an execution, from the initial input to the final output, along with all intermediate steps and metadata.\n",
    "\n",
    "## Observations\n",
    "\n",
    "Each trace contains multiple observations that log individual steps of execution.\n",
    "Observations provide granular visibility into what happens during a request, enabling detailed debugging and performance optimization.\n",
    "\n",
    "They automatically nest through OpenTelemetry context propagation - each new observation becomes a child of the currently active one.\n",
    "\n",
    "### Observation Types\n",
    "\n",
    "| Type | Purpose |\n",
    "|------|---------|\n",
    "| **event** | Discrete point-in-time occurrences |\n",
    "| **span** | Operations with duration |\n",
    "| **generation** | AI model calls (prompts, tokens, costs) |\n",
    "| **agent** | LLM-guided application flow decisions |\n",
    "| **tool** | External API/service calls |\n",
    "| **chain** | Links between application steps |\n",
    "| **retriever** | Data retrieval (vector stores, databases) |\n",
    "| **evaluator** | Assessment of LLM output quality |\n",
    "| **embedding** | Embedding generation with metrics |\n",
    "| **guardrail** | Protection against malicious content |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CallbackHandler for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the weather in Miami?\"\n",
    "config = {\"callbacks\": [langfuse_handler]}\n",
    "input_state = State(messages=[user_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke(input_state, config=config)\n",
    "print(result.get('messages')[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Langfuse automatic trace in Langfuse UI\n",
    "\n",
    "LangChain's hierarchical execution naturally maps to Langfuse's trace structure:\n",
    "\n",
    "- Runnable.invoke() creates a TRACE\n",
    "- Nested chains/agents create child CHAIN\n",
    "- LLM calls create GENERATION observations\n",
    "- Tool calls create TOOL observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![automatic_trace](langgraph_automatic_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collect and Read a Langfuse Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_json(data, output_filename=\"output.json\"):\n",
    "    print(\"saving\")\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(data.json(indent=4))  \n",
    "\n",
    "def extract(trace_id):\n",
    "    langfuse = get_client()\n",
    "    trace = langfuse.api.trace.get(trace_id=trace_id)\n",
    "    trace_dict = trace.dict()\n",
    "    \n",
    "    # Exclude ChannelWrite observations. \n",
    "    # If ChannelWrite are desired, simply set \"exclude_channel_write = False\" bellow\n",
    "    \n",
    "    # Information : ChannelWrite nodes are used internally by LangGraph to \n",
    "    #               manage the flow of information and updates between different \n",
    "    #               nodes or agents within a graph\n",
    "    \n",
    "    exclude_channel_write = True\n",
    "    \n",
    "    if exclude_channel_write:\n",
    "        _observation = []\n",
    "        for observation in trace.observations:\n",
    "            if \"ChannelWrite\" not in observation.name:\n",
    "                _observation.append(observation)\n",
    "        \n",
    "        trace_dict['observations'] = [_observation[0]]\n",
    "        trace = TraceWithFullDetails(\n",
    "            **trace_dict\n",
    "        )\n",
    "\n",
    "    save_as_json(trace, f\"./trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the **trace_id** of the last agent execution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = langfuse_handler.last_trace_id\n",
    "print(f\"Last trace ID: {trace_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Extracting for trace {trace_id}\")\n",
    "    extract(trace_id)\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('trace.json') as f:\n",
    "    trace_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a Langfuse Trace as JSON\n",
    "\n",
    "Here are a few tips and tricks to explore your trace data if you have extracted it as a JSON file.\n",
    "\n",
    "#### Understanding the Trace Structure\n",
    "\n",
    "A Langfuse trace JSON contains several key sections:\n",
    "\n",
    "##### 1. **Top-Level Trace Information**\n",
    "- `id`: Unique identifier for the trace\n",
    "- `name`: Name of the traced operation (e.g., \"LangGraph\")\n",
    "- `timestamp`: When the trace was created\n",
    "- `latency`: Total execution time in seconds\n",
    "\n",
    "##### 2. **Input and Output (Messages)**\n",
    "\n",
    "The trace's `input` and `output` fields contain the conversation flow:\n",
    "\n",
    "**Input** (`trace.input`):\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    \"What is the weather in Miami?\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "This represents the initial user query.\n",
    "\n",
    "**Output** (`trace.output`):\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {...},  // Human message\n",
    "    {...},  // AI message with tool call\n",
    "    {...},  // Tool response\n",
    "    {...}   // Final AI response\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The output contains a **list of messages** representing the complete conversation flow:\n",
    "\n",
    "1. **Human Message** (`type: \"human\"`):\n",
    "   - Contains the user's question\n",
    "   - Has a unique `id` for reference\n",
    "\n",
    "2. **AI Message with Tool Call** (`type: \"ai\"`):\n",
    "   - The model's decision to use a tool\n",
    "   - `tool_calls`: Array of tools the model wants to invoke\n",
    "   - `response_metadata`: Contains token usage and model information\n",
    "   - Example:\n",
    "     ```json\n",
    "     \"tool_calls\": [\n",
    "       {\n",
    "         \"name\": \"get_current_weather\",\n",
    "         \"args\": {\"location\": \"Miami\"},\n",
    "         \"id\": \"chatcmpl-tool-ca1b90c760644de4a5221c48c3c19605\"\n",
    "       }\n",
    "     ]\n",
    "     ```\n",
    "\n",
    "3. **Tool Message** (`type: \"tool\"`):\n",
    "   - Contains the tool's execution result\n",
    "   - `content`: The actual data returned by the tool\n",
    "   - `tool_call_id`: Links back to the tool call that triggered it\n",
    "   - Example:\n",
    "     ```json\n",
    "     {\n",
    "       \"content\": \"{\\\\\"description\\\\\": \\\\\"few clouds\\\\\", \\\\\"temperature\\\\\": 12.98, \\\\\"humidity\\\\\": 41}\",\n",
    "       \"type\": \"tool\",\n",
    "       \"name\": \"get_current_weather\",\n",
    "       \"tool_call_id\": \"chatcmpl-tool-ca1b90c760644de4a5221c48c3c19605\"\n",
    "     }\n",
    "     ```\n",
    "     To find the output of a tool_call execution, locate the observation who's output message has a tool_call_id equal to the id value of the tool_call you are interested in.\n",
    "\n",
    "\n",
    "4. **Final AI Message** (`type: \"ai\"`):\n",
    "   - The model's final response using the tool results\n",
    "   - `content`: Human-readable answer\n",
    "   - `finish_reason: \"stop\"`: Indicates completion\n",
    "\n",
    "##### 3. **Observations**\n",
    "\n",
    "The `observations` array (`trace.observations`) contains detailed execution steps:\n",
    "\n",
    "- **Types**: `GENERATION`, `SPAN`, `CHAIN`, `EVENT`\n",
    "- **Key Fields**:\n",
    "  - `name`: Operation name (e.g., \"route_tools\", \"llm\")\n",
    "  - `input`: Data sent to this step\n",
    "  - `output`: Data returned from this step\n",
    "  - `metadata`: Additional context (LangGraph step info, node names, etc.)\n",
    "  - `startTime` / `endTime`: Timing information\n",
    "  - `parentObservationId`: Links to parent observation for hierarchy\n",
    "\n",
    "**Important Notes**:\n",
    "- `GENERATION` observations are listed in **reverse chronological order**\n",
    "- Each observation has a unique `id` and can be matched to UI traces\n",
    "- The `input` field in observations often contains the full message history up to that point\n",
    "\n",
    "#### Reading the Message Flow\n",
    "\n",
    "To understand the execution flow:\n",
    "\n",
    "1. Start with `trace.input.messages` for the initial query\n",
    "2. Follow `trace.output.messages` sequentially to see:\n",
    "   - User question → AI tool call → Tool execution → AI final answer\n",
    "3. Cross-reference with `trace.observations` for detailed execution metadata\n",
    "4. Match tool calls using `tool_call_id` to link requests and responses\n",
    "\n",
    "#### Example: Tracing a Weather Query\n",
    "\n",
    "For the query \"What is the weather in Miami?\":\n",
    "\n",
    "```\n",
    "Input: \"What is the weather in Miami?\"\n",
    "  ↓\n",
    "AI decides to call: get_current_weather(location=\"Miami\")\n",
    "  ↓\n",
    "Tool returns: {\"description\": \"few clouds\", \"temperature\": 12.98, \"humidity\": 41}\n",
    "  ↓\n",
    "AI responds: \"The current weather in Miami is few clouds with a temperature of 12.98°C...\"\n",
    "```\n",
    "\n",
    "All of this is captured in the `output.messages` array with proper type annotations and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_calls(trace_data):\n",
    "    \"\"\"\n",
    "    Extract tool calls and their corresponding results from a Langfuse trace.\n",
    "    \n",
    "    Tool calls are found in AI messages that have a non-empty 'tool_calls' list.\n",
    "    Tool call results are matched by tool_call_id, searching first in the trace\n",
    "    output messages, then falling back to the observations.\n",
    "    \n",
    "    Returns a list of dicts with keys: name, args, tool_call_id, result\n",
    "    \"\"\"\n",
    "    messages = trace_data.get(\"output\", {}).get(\"messages\", [])\n",
    "    observations = trace_data.get(\"observations\", [])\n",
    "\n",
    "    # Collect all tool calls from AI messages\n",
    "    tool_calls = []\n",
    "    for msg in messages:\n",
    "        if msg.get(\"type\") in (\"ai\", \"assistant\") and msg.get(\"tool_calls\"):\n",
    "            for tc in msg[\"tool_calls\"]:\n",
    "                tool_calls.append({\n",
    "                    \"name\": tc[\"name\"],\n",
    "                    \"args\": tc[\"args\"],\n",
    "                    \"tool_call_id\": tc[\"id\"],\n",
    "                    \"result\": None,\n",
    "                })\n",
    "\n",
    "    for tc in tool_calls:\n",
    "        tid = tc[\"tool_call_id\"]\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg.get(\"type\") == \"tool\" and msg.get(\"tool_call_id\") == tid:\n",
    "                tc[\"result\"] = msg.get(\"content\")\n",
    "                break\n",
    "\n",
    "        if tc[\"result\"] is None:\n",
    "            for obs in observations:\n",
    "                obs_output = obs.get(\"output\", {}) or {}\n",
    "                obs_meta = obs.get(\"metadata\", {}) or {}\n",
    "                if (obs_output.get(\"tool_call_id\") == tid\n",
    "                        or obs_meta.get(\"tool_call_id\") == tid):\n",
    "                    tc[\"result\"] = obs_output.get(\"content\")\n",
    "                    break\n",
    "\n",
    "    return tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = extract_tool_calls(trace_json)\n",
    "\n",
    "for tc in tool_calls:\n",
    "    print(f\"Tool:    {tc['name']}\")\n",
    "    print(f\"Args:    {tc['args']}\")\n",
    "    print(f\"Call ID: {tc['tool_call_id']}\")\n",
    "    print(f\"Result:  {tc['result']}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Send a Trace to langfuse Manually\n",
    "\n",
    "### Managing Langfuse observations with a simple nested trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with langfuse_client.start_as_current_observation(\n",
    "    as_type=\"chain\",\n",
    "    name=\"example_agent\",\n",
    "    input={\"query\": \"What's the weather in San Francisco?\"}\n",
    ") as root:\n",
    "    \n",
    "    with langfuse_client.start_as_current_observation(\n",
    "        as_type=\"generation\",\n",
    "        name=\"llm\",\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]},\n",
    "        model=model\n",
    "    ) as gen:\n",
    "        gen.update(\n",
    "            output={\"content\": \"I'll check the weather in San Francisco.\"},\n",
    "            usage={\"input\": 10, \"output\": 5, \"total\": 15}\n",
    "        )\n",
    "    with langfuse_client.start_as_current_observation(\n",
    "        as_type=\"tool\",\n",
    "        name=\"weather_tool\",\n",
    "        input={\"location\": \"San Francisco\"}\n",
    "    ) as tool:\n",
    "        tool.update(output={\"temp\": 72, \"conditions\": \"sunny\"})\n",
    "\n",
    "    root.update(output={\"response\": \"It's 72°F and sunny\"})\n",
    "\n",
    "langfuse_client.flush()\n",
    "\n",
    "print(f\"Trace ID: {root.trace_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![automatic_trace](langgraph_simple_manual_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Langfuse observations for LangGraph events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangfuseObservationTracker:\n",
    "    \"\"\"\n",
    "    Simplified observation tracker for educational purposes.\n",
    "    \n",
    "    This class demonstrates basic Langfuse tracing concepts:\n",
    "    - Creating generations (LLM calls)\n",
    "    - Creating tool observations (tool executions)\n",
    "    \n",
    "    Uses langfuse_client.start_as_current_observation() to create child\n",
    "    observations. Nesting is automatic through OpenTelemetry context\n",
    "    propagation — any observation created within the root trace's context\n",
    "    manager becomes a child of that trace.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, langfuse_client):\n",
    "        \"\"\"\n",
    "        Initialize the tracker with a Langfuse client.\n",
    "        \n",
    "        Args:\n",
    "            langfuse_client: The Langfuse client instance used to create observations\n",
    "        \"\"\"\n",
    "        self.langfuse_client = langfuse_client\n",
    "\n",
    "    def track_llm_call(self, name: str, input_messages: Any, output: Any, model: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Track an LLM call as a Langfuse generation.\n",
    "        \n",
    "        A generation is a specialized observation type for AI model interactions.\n",
    "        It captures the input prompt, output response, and model information.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the LLM call (e.g., \"ChatModel\")\n",
    "            input_messages: The input messages sent to the LLM\n",
    "            output: The response from the LLM\n",
    "            model: Optional model name (e.g., \"ibm/granite-4-h-small\")\n",
    "        \"\"\"\n",
    "        print(f\"  Tracking LLM call: {name}\")\n",
    "        \n",
    "        with self.langfuse_client.start_as_current_observation(\n",
    "            as_type=\"generation\",\n",
    "            name=name,\n",
    "            input=input_messages,\n",
    "            model=model,\n",
    "        ) as generation:\n",
    "            generation.update(output=output)\n",
    "        return generation\n",
    "\n",
    "    def track_tool_call(self, name: str, tool_input: Any, tool_output: Any):\n",
    "        \"\"\"\n",
    "        Track a tool execution as a Langfuse tool observation.\n",
    "        \n",
    "        A tool observation captures when the agent calls an external tool/function.\n",
    "        It records what tool was called, with what input, and what it returned.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the tool (e.g., \"get_current_weather\")\n",
    "            tool_input: The input arguments passed to the tool\n",
    "            tool_output: The result returned by the tool\n",
    "        \"\"\"\n",
    "        print(f\"  Tracking tool call: {name}\")\n",
    "        \n",
    "        with self.langfuse_client.start_as_current_observation(\n",
    "            as_type=\"tool\",\n",
    "            name=name,\n",
    "            input=tool_input,\n",
    "        ) as tool_obs:\n",
    "            tool_obs.update(output=tool_output)\n",
    "        return tool_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_event_generator(graph: CompiledStateGraph, user_input: str):\n",
    "    \"\"\"\n",
    "    manual Langfuse tracing from our langgraph graph execution.\n",
    "    \n",
    "    Steps:\n",
    "    1. Create root trace - container for all observations, represents entire agent execution\n",
    "    2. Stream agent events - capture step-by-step graph node executions\n",
    "    3. Log observations - LLM calls as \"generation\", tool calls as \"tool\"\n",
    "    4. Flush to Langfuse - ensure all data is sent to server\n",
    "    \"\"\"\n",
    "\n",
    "    user_message = HumanMessage(user_input)\n",
    "    input_state = State(messages=[user_message])\n",
    "    session_id = str(uuid.uuid4())\n",
    "\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Session ID: {session_id}\\n\")\n",
    "\n",
    "    print(\"STEP 1: Creating root trace...\")\n",
    "\n",
    "    with langfuse_client.start_as_current_observation(\n",
    "        as_type=\"chain\",\n",
    "        name=\"Simple_Agent_Trace\",\n",
    "        input={\"user_message\": user_input},\n",
    "        metadata={\"session_id\": session_id}\n",
    "    ) as trace:\n",
    "        print(f\"Trace created with ID: {trace.trace_id}\\n\")\n",
    "\n",
    "        tracker = LangfuseObservationTracker(langfuse_client)\n",
    "\n",
    "        print(\"STEP 2: Executing agent and capturing events...\\n\")\n",
    "\n",
    "        final_output = None\n",
    "        llm_call_count = 0\n",
    "        tool_call_count = 0\n",
    "\n",
    "        # event is a dict with node_name as key and state update as value\n",
    "        for event in graph.stream(input_state):\n",
    "            for node_name, state_update in event.items():\n",
    "                print(f\"Node executed: {node_name}\")\n",
    "\n",
    "                if node_name == \"llm\":\n",
    "                    llm_call_count += 1\n",
    "                    messages = state_update.get(\"messages\", [])\n",
    "                    if messages:\n",
    "                        last_message = messages[-1]\n",
    "\n",
    "                        input_msgs = input_state.get(\"messages\", []) if llm_call_count == 1 else []\n",
    "                        output_content = last_message.content if hasattr(last_message, \"content\") else str(last_message)\n",
    "\n",
    "                        tracker.track_llm_call(\n",
    "                            name=f\"LLM_Call_{llm_call_count}\",\n",
    "                            input_messages=input_msgs,\n",
    "                            output={\"content\": output_content},\n",
    "                            model=model\n",
    "                        )\n",
    "\n",
    "                        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "                            print(f\"  LLM decided to call {len(last_message.tool_calls)} tool(s)\")\n",
    "                        else:\n",
    "                            print(f\"  LLM generated final response\")\n",
    "\n",
    "                elif node_name == \"tools\":\n",
    "                    tool_call_count += 1\n",
    "                    messages = state_update.get(\"messages\", [])\n",
    "                    if messages:\n",
    "                        for msg in messages:\n",
    "                            if hasattr(msg, \"name\"):\n",
    "                                tool_name = msg.name\n",
    "                                tool_output = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "                                tracker.track_tool_call(\n",
    "                                    name=tool_name,\n",
    "                                    tool_input={\"call\": f\"Tool call #{tool_call_count}\"},\n",
    "                                    tool_output={\"result\": tool_output}\n",
    "                                )\n",
    "                                print(f\"  Tool '{tool_name}' executed\")\n",
    "                final_output = state_update\n",
    "\n",
    "        if final_output and \"messages\" in final_output:\n",
    "            final_message = final_output[\"messages\"][-1]\n",
    "            final_response = final_message.content if hasattr(final_message, \"content\") else str(final_message)\n",
    "        else:\n",
    "            final_response = \"No response generated\"\n",
    "\n",
    "        print(\"\\nSTEP 4: Finalizing trace...\")\n",
    "        trace.update(output={\"final_response\": final_response})\n",
    "\n",
    "    langfuse_client.flush()\n",
    "    print(\"Trace data sent to Langfuse\\n\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    print(\"EXECUTION SUMMARY:\")\n",
    "    print(f\"  LLM calls: {llm_call_count}\")\n",
    "    print(f\"  Tool calls: {tool_call_count}\")\n",
    "    print(f\"  Final response: {final_response[:100]}...\")\n",
    "    print(f\"  Trace ID: {trace.trace_id}\")\n",
    "    print(f\"  View in Langfuse: {os.environ.get('LANGFUSE_HOST', 'https://cloud.langfuse.com')}/trace/{trace.trace_id}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"session_id\": session_id,\n",
    "        \"output\": final_response,\n",
    "        \"trace_id\": trace.trace_id,\n",
    "        \"llm_calls\": llm_call_count,\n",
    "        \"tool_calls\": tool_call_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the weather in Miami?\"\n",
    "test_event_generator(graph=graph,user_input=user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "![manual_trace](langgraph_stream_manual_trace.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
