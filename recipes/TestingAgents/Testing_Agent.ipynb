{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Evaluation with Langfuse Tracing\n",
    "\n",
    "This notebook demonstrates how to evaluate AI agents using **Langfuse** for observability and tracing.\n",
    "\n",
    "1. **Langfuse Setup** - Installing and configuring Langfuse for tracing\n",
    "2. **Agent Implementation** - Building a simple agent\n",
    "3. **Manual Trace Capture** - Capture a trace manually and how to read it\n",
    "4. **Automatic Trace Capture** - Capture a trace with callback handler.\n",
    "5. **Test Dataset Creation** - Defining test cases with expected behaviors\n",
    "6. **Trajectory Evaluation** - Validating tool calls and execution paths\n",
    "7. **Response Evaluation** - Assessing agent responses using LLM-as-a-Judge\n",
    "\n",
    "## Why Agent Evaluation?\n",
    "\n",
    "Before increasing the complexity of your agent architecture, you should validate the need for additions. \n",
    "\n",
    "This is done by:\n",
    "\n",
    "- Capture agent traces\n",
    "- Defining a test set capturing desired use cases\n",
    "- Running evaluations against the agent\n",
    "- Measuring trajectory accuracy and response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langfuse langchain langgraph langchain_ibm pandas python-dotenv matplotlib seaborn\n",
    "!pip install -q \"git+https://github.com/ibm-granite-community/utils.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langfuse import Langfuse\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "from typing import Any, Dict\n",
    "from typing import Annotated, TypedDict\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain_core.messages import AnyMessage, AIMessage, HumanMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model = \"ibm/granite-4-h-small\"\n",
    "\n",
    "model_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse Setup\n",
    "\n",
    "#### Launch Langfuse with Docker\n",
    "\n",
    "Before running this notebook, start Langfuse locally:\n",
    "\n",
    "```bash\n",
    "# Clone Langfuse repository\n",
    "git clone https://github.com/langfuse/langfuse.git\n",
    "cd langfuse\n",
    "\n",
    "# Start Langfuse with Docker Compose\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "### Configure Langfuse\n",
    "\n",
    "1. Open http://localhost:3000 in your browser\n",
    "2. Sign up or log in\n",
    "3. Create a new project (e.g., \"Agent Evaluation\")\n",
    "4. Navigate to **Settings → API Keys**\n",
    "5. Create new API keys and copy:\n",
    "   - Public Key\n",
    "   - Secret Key\n",
    "   - Host URL (http://localhost:3000)\n",
    "\n",
    "### Configure watsonx.ai\n",
    "\n",
    "You'll need the following environment variables for watsonx.ai:\n",
    "- `WATSONX_URL`: Your watsonx.ai endpoint URL\n",
    "- `WATSONX_APIKEY`: Your watsonx.ai API key\n",
    "- `WATSONX_PROJECT_ID`: Your watsonx.ai project ID\n",
    "\n",
    "Set these in a `.env` file or export them as environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Langfuse Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Langfuse client\n",
    "langfuse_client = Langfuse(\n",
    "    public_key=os.environ['LANGFUSE_PUBLIC_KEY'],\n",
    "    secret_key=os.environ['LANGFUSE_SECRET_KEY'],\n",
    "    host=os.environ['LANGFUSE_HOST']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agent Implementation\n",
    "\n",
    "### Build a Minimal Function Calling Agent\n",
    "\n",
    "Create the FC agent using LangChain's `create_agent` from [Function_Calling](https://github.com/ibm-granite-community/granite-agent-cookbook/blob/main/recipes/Function_Calling/Function_Calling_Agent.ipynb) recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_STOCK_API_KEY = convert_to_secret_str(get_env_var(\"AV_STOCK_API_KEY\", \"unset\"))\n",
    "\n",
    "WEATHER_API_KEY = convert_to_secret_str(get_env_var(\"WEATHER_API_KEY\", \"unset\"))\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=model_parameters,\n",
    ")\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def get_stock_price(ticker: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the lowest and highest stock prices for a given ticker and date.\n",
    "\n",
    "    Args:\n",
    "        ticker: The stock ticker symbol, for example, \"IBM\".\n",
    "        date: The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the low and high stock prices on the given date.\n",
    "    \"\"\"\n",
    "    print(f\"Getting stock price for {ticker} on {date}\")\n",
    "\n",
    "    apikey = AV_STOCK_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"low\": \"245.4500\",\n",
    "            \"high\": \"249.0300\"\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={apikey}\"\n",
    "        stock_data = requests.get(stock_url)\n",
    "        data = stock_data.json()\n",
    "        stock_low = data[\"Time Series (Daily)\"][date][\"3. low\"]\n",
    "        stock_high = data[\"Time Series (Daily)\"][date][\"2. high\"]\n",
    "        return {\n",
    "            \"low\": stock_low,\n",
    "            \"high\": stock_high\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return {\n",
    "            \"low\": \"$245.45\",\n",
    "            \"high\": \"$249.03\"\n",
    "        }\n",
    "\n",
    "\n",
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location (default: San Francisco).\n",
    "\n",
    "    Args:\n",
    "        location: The name of the city for which to retrieve the weather information.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing weather information such as temperature in celsius, weather description, and humidity.\n",
    "    \"\"\"\n",
    "    print(f\"Getting current weather for {location}\")\n",
    "    apikey=WEATHER_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"description\": \"thunderstorms\",\n",
    "            \"temperature\": 25.3,\n",
    "            \"humidity\": 94\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # API request to fetch weather data\n",
    "        weather_url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={apikey}&units=metric\"\n",
    "        weather_data = requests.get(weather_url)\n",
    "        data = weather_data.json()\n",
    "        # Extracting relevant weather details\n",
    "        weather_description = data[\"weather\"][0][\"description\"]\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "\n",
    "        # Returning weather details\n",
    "        return {\n",
    "            \"description\": weather_description,\n",
    "            \"temperature\": temperature,\n",
    "            \"humidity\": humidity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return {\n",
    "            \"description\": \"none\",\n",
    "            \"temperature\": \"none\",\n",
    "            \"humidity\": \"none\"\n",
    "        }\n",
    "\n",
    "\n",
    "def route_tools(state: State) -> str:\n",
    "    \"\"\"\n",
    "    This is conditional_edge function to route to the ToolNode if the last message\n",
    "    in the state has tool calls. Otherwise, route to the END node to complete the\n",
    "    workflow.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\")\n",
    "    if not messages:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    # If the last message is from the model and it contains a tool call request\n",
    "    if isinstance(last_message, AIMessage) and len(last_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"Create and return the LLM instance.\"\"\"\n",
    "    return init_chat_model(\n",
    "        model=MODEL,\n",
    "        model_provider=\"ibm\",\n",
    "        url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "        apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "        project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "        params=MODEL_PARAMETERS,\n",
    "    )\n",
    "\n",
    "def llm_node(state: State) -> State:\n",
    "    messages = state[\"messages\"]\n",
    "    response_message = llm_with_tools.invoke(messages)\n",
    "    state_update = State(messages=[response_message])\n",
    "    return state_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_stock_price, get_current_weather]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"llm\", llm_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_edge(START, \"llm\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "\n",
    "graph: CompiledStateGraph[State] = graph_builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "def function_calling_agent(graph: CompiledStateGraph, user_input: str):\n",
    "    user_message = HumanMessage(user_input)\n",
    "    print(user_message.pretty_repr())\n",
    "    input = State(messages=[user_message])\n",
    "    for event in graph.stream(input):\n",
    "        for value in event.values():\n",
    "            print(value[\"messages\"][-1].pretty_repr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_calling_agent(graph, \"What is the weather in Miami?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Langfuse experiment based on the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "from langfuse import Evaluation\n",
    "\n",
    "def my_task(*, item, **kwargs):\n",
    "    user_message = HumanMessage(item[\"input\"])\n",
    "    response = graph.invoke( {\"messages\": [user_message]})\n",
    "    return response.get('messages')[-1].content\n",
    "    \n",
    "    \n",
    "# Initialize client\n",
    "langfuse = get_client()\n",
    "\n",
    "# Run experiment on local data\n",
    "local_data = [\n",
    "    {\"input\": \"What were the IBM stock prices on September 5, 2025?\", \"expected_output\": \"On September 5, 2025, the stock price of IBM ranged from a low of $245.45 to a high of $249.03.\"},\n",
    "]\n",
    "\n",
    "# Define evaluation functions\n",
    "def accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\n",
    "    if expected_output and expected_output.lower() in output.lower():\n",
    "        return Evaluation(name=\"accuracy\", value=1.0, comment=\"Correct answer found\")\n",
    "\n",
    "    return Evaluation(name=\"accuracy\", value=0.0, comment=\"Incorrect answer\")\n",
    "\n",
    "\n",
    "def length_evaluator(*, input, output, **kwargs):\n",
    "    return Evaluation(name=\"response_length\", value=len(output), comment=f\"Response has {len(output)} characters\")\n",
    "    \n",
    "result = langfuse.run_experiment(\n",
    "    name=\"Stock Market Quizz\",\n",
    "    description=\"Testing basic functionality\",\n",
    "    data=local_data,\n",
    "    task=my_task,\n",
    "    evaluators=[accuracy_evaluator, length_evaluator]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review experiment in Langfuse UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the experiment, you can view the results in the Langfuse dashboard.\n",
    "The experiment tracked two key metrics:\n",
    "\n",
    "**Accuracy**: Whether the model's response contained the expected stock price information\n",
    "\n",
    "**Response Length**: The character count of each response, helping assess verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![experiment](experiment.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Send a Trace to langfuse Manually\n",
    "\n",
    "### Managing Langfuse observations for LangGraph events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langfuse defines three types of observations, each serving a specific purpose:\n",
    "\n",
    "1. **Events**\n",
    "\n",
    "Events are the basic building blocks used to track discrete occurrences in a trace. They represent point-in-time actions without duration.\n",
    "\n",
    "Use cases:\n",
    "- Logging custom application events\n",
    "- Recording state changes\n",
    "- Tracking decision points or checkpoints\n",
    "\n",
    "2. **Spans**\n",
    "\n",
    "Spans represent durations of units of work in a trace.\n",
    "They track operations that have a start time and end time, providing performance insights.\n",
    "\n",
    "Common span types:\n",
    "- Chain executions (sequences of operations)\n",
    "- Tool invocations (external API calls, database queries)\n",
    "- Retriever operations (vector database searches)\n",
    "- Agent decision loops\n",
    "\n",
    "3. **Generations**\n",
    "\n",
    "Generations are specialized spans designed specifically for AI model interactions.\n",
    "\n",
    "They capture comprehensive details about language model calls.\n",
    "\n",
    "Generation details include:\n",
    "\n",
    "- Model name and version\n",
    "- Model parameters (temperature, max tokens, top-p)\n",
    "- Complete prompt or message history\n",
    "- Generated output text\n",
    "- Token usage (input tokens, output tokens, total tokens)\n",
    "- Cost calculation based on token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangfuseObservationTracker:\n",
    "    \"\"\"\n",
    "    Observation tracker\n",
    "\n",
    "    This class manages Langfuse observations for LangGraph events,\n",
    "    tracking run IDs, parent relationships, and observation lifecycles\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, client, root_span=None):\n",
    "        \"\"\"\n",
    "        Initialize the observation tracker.\n",
    "\n",
    "        Args:\n",
    "            client: The Langfuse client instance\n",
    "            root_span: Optional root span for the trace\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.root_span = root_span\n",
    "\n",
    "        # Track active observations by run_id (mirrors LangChain's self.runs)\n",
    "        self.runs: Dict[str, Any] = {}\n",
    "\n",
    "        # Track parent-child relationships (mirrors LangChain's _child_to_parent_run_id_map)\n",
    "        self._child_to_parent_run_id_map: Dict[str, Optional[str]] = {}\n",
    "\n",
    "        # Track context managers for proper cleanup\n",
    "        self.context_managers: Dict[str, Any] = {}\n",
    "\n",
    "        # Track completion start times for streaming (mirrors LangChain's updated_completion_start_time_memo)\n",
    "        self.updated_completion_start_time_memo: set = set()\n",
    "\n",
    "        # Store the last trace ID\n",
    "        self.last_trace_id: Optional[str] = None\n",
    "\n",
    "    def _get_observation_type(self, event_type: str, name: str, serialized: Optional[Dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        Determine Langfuse observation type from event type and name.\n",
    "\n",
    "        Args:\n",
    "            event_type: The LangGraph event type\n",
    "            name: The component name\n",
    "            serialized: Optional serialized component data\n",
    "\n",
    "        Returns:\n",
    "            The appropriate Langfuse observation type\n",
    "        \"\"\"\n",
    "        if \"tool\" in event_type:\n",
    "            return \"tool\"\n",
    "        elif \"chat_model\" in event_type or \"llm\" in event_type:\n",
    "            return \"generation\"\n",
    "        elif \"retriever\" in event_type:\n",
    "            return \"retriever\"\n",
    "        elif \"chain\" in event_type:\n",
    "            # Check if it's an agent\n",
    "            if \"agent\" in name.lower():\n",
    "                return \"agent\"\n",
    "            return \"chain\"\n",
    "        return \"span\"\n",
    "\n",
    "    def _get_parent_observation(self, parent_run_id: Optional[str]):\n",
    "        \"\"\"\n",
    "        Get the parent observation for nesting.\n",
    "\n",
    "        Args:\n",
    "            parent_run_id: The parent run ID\n",
    "\n",
    "        Returns:\n",
    "            The parent observation or client for starting new observations\n",
    "        \"\"\"\n",
    "        if parent_run_id and parent_run_id in self.runs:\n",
    "            return self.runs[parent_run_id]\n",
    "        if self.root_span:\n",
    "            return self.root_span\n",
    "        return self.client\n",
    "\n",
    "    def _attach_observation(self, run_id: str, observation, context_manager=None):\n",
    "        \"\"\"\n",
    "        Store an observation for tracking.\n",
    "\n",
    "        Args:\n",
    "            run_id: The run ID\n",
    "            observation: The Langfuse observation\n",
    "            context_manager: Optional context manager for cleanup\n",
    "        \"\"\"\n",
    "        self.runs[run_id] = observation\n",
    "        if context_manager:\n",
    "            self.context_managers[run_id] = context_manager\n",
    "\n",
    "        if hasattr(observation, 'trace_id'):\n",
    "            self.last_trace_id = observation.trace_id\n",
    "\n",
    "    def _detach_observation(self, run_id: str):\n",
    "        \"\"\"\n",
    "        Remove and return an observation.\n",
    "\n",
    "        Args:\n",
    "            run_id: The run ID\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (observation, context_manager) or (None, None)\n",
    "        \"\"\"\n",
    "        observation = self.runs.pop(run_id, None)\n",
    "        context_manager = self.context_managers.pop(run_id, None)\n",
    "        return observation, context_manager\n",
    "\n",
    "    def on_chain_start(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        name: str,\n",
    "        inputs: Any,\n",
    "        parent_run_id: Optional[str] = None,\n",
    "        metadata: Optional[Dict] = None,\n",
    "        tags: Optional[list] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle chain/node start event.\n",
    "        \"\"\"\n",
    "        self._child_to_parent_run_id_map[run_id] = parent_run_id\n",
    "\n",
    "        obs_type = self._get_observation_type(\"chain\", name)\n",
    "\n",
    "        # Build metadata\n",
    "        span_metadata = {}\n",
    "        if tags:\n",
    "            span_metadata[\"tags\"] = tags\n",
    "        if metadata:\n",
    "            span_metadata.update(metadata)\n",
    "\n",
    "        # Create observation\n",
    "        obs_context = self.client.start_as_current_observation(\n",
    "            as_type=obs_type,\n",
    "            name=name,\n",
    "            input=inputs,\n",
    "            metadata=span_metadata if span_metadata else None,\n",
    "        )\n",
    "        obs = obs_context.__enter__()\n",
    "        self._attach_observation(run_id, obs, obs_context)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def on_chain_end(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        outputs: Any,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle chain/node end event.\n",
    "        \"\"\"\n",
    "        obs, obs_context = self._detach_observation(run_id)\n",
    "\n",
    "        if obs is not None:\n",
    "            obs.update(output=outputs)\n",
    "            if obs_context:\n",
    "                obs_context.__exit__(None, None, None)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def on_chain_error(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        error: BaseException,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle chain/node error event.\n",
    "        \"\"\"\n",
    "        obs, obs_context = self._detach_observation(run_id)\n",
    "\n",
    "        if obs is not None:\n",
    "            obs.update(\n",
    "                level=\"ERROR\",\n",
    "                status_message=str(error),\n",
    "            )\n",
    "            if obs_context:\n",
    "                obs_context.__exit__(None, None, None)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        name: str,\n",
    "        messages: Any,\n",
    "        parent_run_id: Optional[str] = None,\n",
    "        model_name: Optional[str] = None,\n",
    "        model_parameters: Optional[Dict] = None,\n",
    "        metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle LLM/chat model start event.\n",
    "        \"\"\"\n",
    "        self._child_to_parent_run_id_map[run_id] = parent_run_id\n",
    "\n",
    "        obs_context = self.client.start_as_current_observation(\n",
    "            as_type=\"generation\",\n",
    "            name=name,\n",
    "            input=messages,\n",
    "            model=model_name,\n",
    "            model_parameters=model_parameters,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        obs = obs_context.__enter__()\n",
    "        self._attach_observation(run_id, obs, obs_context)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def on_chat_model_end(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        output: Any,\n",
    "        usage: Optional[Dict] = None,\n",
    "        model: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle LLM/chat model end event.\n",
    "        \"\"\"\n",
    "        obs, obs_context = self._detach_observation(run_id)\n",
    "\n",
    "        if obs is not None:\n",
    "            update_kwargs = {\"output\": output}\n",
    "            if usage:\n",
    "                update_kwargs[\"usage\"] = usage\n",
    "                update_kwargs[\"usage_details\"] = usage\n",
    "            if model:\n",
    "                update_kwargs[\"model\"] = model\n",
    "\n",
    "            obs.update(**update_kwargs)\n",
    "            if obs_context:\n",
    "                obs_context.__exit__(None, None, None)\n",
    "\n",
    "        # Clean up streaming memo\n",
    "        self.updated_completion_start_time_memo.discard(run_id)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def on_llm_new_token(self, run_id: str):\n",
    "        \"\"\"\n",
    "        Handle streaming token event - update completion start time.\n",
    "        \"\"\"\n",
    "        if run_id in self.runs and run_id not in self.updated_completion_start_time_memo:\n",
    "            obs = self.runs[run_id]\n",
    "            from langfuse._utils import _get_timestamp\n",
    "            obs.update(completion_start_time=_get_timestamp())\n",
    "            self.updated_completion_start_time_memo.add(run_id)\n",
    "\n",
    "    def on_tool_start(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        name: str,\n",
    "        input_str: Any,\n",
    "        parent_run_id: Optional[str] = None,\n",
    "        metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle tool start event.\n",
    "        \"\"\"\n",
    "        self._child_to_parent_run_id_map[run_id] = parent_run_id\n",
    "\n",
    "        obs_context = self.client.start_as_current_observation(\n",
    "            as_type=\"tool\",\n",
    "            name=name,\n",
    "            input=input_str,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        obs = obs_context.__enter__()\n",
    "        self._attach_observation(run_id, obs, obs_context)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def on_tool_end(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        output: Any,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle tool end event.\n",
    "        \"\"\"\n",
    "        obs, obs_context = self._detach_observation(run_id)\n",
    "\n",
    "        if obs is not None:\n",
    "            obs.update(output=output)\n",
    "            if obs_context:\n",
    "                obs_context.__exit__(None, None, None)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def on_tool_error(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        error: BaseException,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Handle tool error event.\n",
    "        \"\"\"\n",
    "        obs, obs_context = self._detach_observation(run_id)\n",
    "\n",
    "        if obs is not None:\n",
    "            obs.update(\n",
    "                level=\"ERROR\",\n",
    "                status_message=str(error),\n",
    "            )\n",
    "            if obs_context:\n",
    "                obs_context.__exit__(None, None, None)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up any remaining observations.\"\"\"\n",
    "        for run_id in list(self.runs.keys()):\n",
    "            obs, obs_context = self._detach_observation(run_id)\n",
    "            if obs_context:\n",
    "                try:\n",
    "                    obs_context.__exit__(None, None, None)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_usage_from_response(output) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Parse usage information from LLM response.\n",
    "    \"\"\"\n",
    "    if not output:\n",
    "        return None\n",
    "\n",
    "    usage = None\n",
    "\n",
    "    # Try to get from response_metadata\n",
    "    if hasattr(output, \"response_metadata\"):\n",
    "        response_metadata = output.response_metadata\n",
    "\n",
    "        # Standard LangChain format\n",
    "        token_usage = response_metadata.get(\"token_usage\", {})\n",
    "        if token_usage:\n",
    "            usage = {\n",
    "                \"input\": token_usage.get(\"prompt_tokens\", 0),\n",
    "                \"output\": token_usage.get(\"completion_tokens\", 0),\n",
    "                \"total\": token_usage.get(\"total_tokens\", 0),\n",
    "            }\n",
    "\n",
    "        # Alternative formats\n",
    "        if not usage and \"usage\" in response_metadata:\n",
    "            raw_usage = response_metadata[\"usage\"]\n",
    "            if isinstance(raw_usage, dict):\n",
    "                usage = {\n",
    "                    \"input\": raw_usage.get(\"input_tokens\", raw_usage.get(\"prompt_tokens\", 0)),\n",
    "                    \"output\": raw_usage.get(\"output_tokens\", raw_usage.get(\"completion_tokens\", 0)),\n",
    "                    \"total\": raw_usage.get(\"total_tokens\", 0),\n",
    "                }\n",
    "\n",
    "    # Try to get from usage_metadata (Ollama, etc.)\n",
    "    if not usage and hasattr(output, \"usage_metadata\"):\n",
    "        usage_metadata = output.usage_metadata\n",
    "        if usage_metadata:\n",
    "            usage = {\n",
    "                \"input\": getattr(usage_metadata, \"input_tokens\", 0),\n",
    "                \"output\": getattr(usage_metadata, \"output_tokens\", 0),\n",
    "                \"total\": getattr(usage_metadata, \"total_tokens\", 0),\n",
    "            }\n",
    "\n",
    "    return usage\n",
    "\n",
    "\n",
    "def _format_llm_output(output) -> Dict:\n",
    "    \"\"\"\n",
    "    Format LLM output for Langfuse.\n",
    "    \"\"\"\n",
    "    if not output:\n",
    "        return output\n",
    "\n",
    "    if hasattr(output, \"content\"):\n",
    "        formatted = {\"content\": output.content}\n",
    "\n",
    "        if hasattr(output, \"tool_calls\") and output.tool_calls:\n",
    "            formatted[\"tool_calls\"] = [\n",
    "                {\n",
    "                    \"name\": tc.get(\"name\", \"\"),\n",
    "                    \"args\": tc.get(\"args\", {}),\n",
    "                    \"id\": tc.get(\"id\", \"\"),\n",
    "                }\n",
    "                for tc in output.tool_calls\n",
    "            ]\n",
    "            formatted[\"finish_reason\"] = \"tool_calls\"\n",
    "\n",
    "        return formatted\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_event_generator(graph:CompiledStateGraph,user_input:str):\n",
    "    \"\"\"\n",
    "    Async test function with proper Langfuse tracing using callback handler pattern.\n",
    "\n",
    "    This function implements Langfuse tracing:\n",
    "    1. Uses LangfuseObservationTracker to manage observation lifecycles\n",
    "    2. Tracks run IDs and parent relationships\n",
    "    3. Creates proper nested observations (chain → generation → tool)\n",
    "    4. Handles start/end events for each observation type\n",
    "\n",
    "    The tracing pattern mirrors LangChain's approach:\n",
    "    - on_chain_start/end: Creates chain observations for nodes\n",
    "    - on_chat_model_start/end: Creates generation observations for LLM calls\n",
    "    - on_tool_start/end: Creates tool observations for tool executions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TESTING MANUAL TRACING OF AGENT EXECUTION EVENTS\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # Create test input\n",
    "    user_message = HumanMessage(user_input)\n",
    "    input_state = State(messages=[user_message])\n",
    "    session_id = str(uuid.uuid4())\n",
    "\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Session ID: {session_id}\")\n",
    "    print(\"\\nStreaming events with Langfuse tracing...\\n\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # ROOT TRACE: Create the main trace for the entire execution\n",
    "    # =========================================================================\n",
    "    with langfuse_client.start_as_current_observation(\n",
    "        as_type=\"chain\",\n",
    "        name=\"Manual_Trace\",\n",
    "        input={\"messages\": [{\"content\": user_input, \"type\": \"human\"}]},\n",
    "        metadata={\"session_id\": session_id}\n",
    "    ) as root_span:\n",
    "\n",
    "        # Initialize observation tracker\n",
    "        tracker = LangfuseObservationTracker(langfuse_client, root_span)\n",
    "\n",
    "        final_output = None\n",
    "        step_count = 0\n",
    "\n",
    "        # =====================================================================\n",
    "        # STREAM EVENTS: Process events\n",
    "        # =====================================================================\n",
    "        async for event in graph.astream_events(input_state, version=\"v2\"):\n",
    "            event_type = event[\"event\"]\n",
    "            name = event.get(\"name\", \"unknown\")\n",
    "            run_id = event.get(\"run_id\", \"\")\n",
    "            parent_ids = event.get(\"parent_ids\", [])\n",
    "            data = event.get(\"data\", {})\n",
    "            metadata = event.get(\"metadata\", {})\n",
    "            tags = event.get(\"tags\", [])\n",
    "\n",
    "            # Determine parent run_id from parent_ids list\n",
    "            parent_run_id = parent_ids[-1] if parent_ids else None\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_CHAIN_START\n",
    "            # =================================================================\n",
    "            if event_type == \"on_chain_start\":\n",
    "                # Skip root graph (already have root_span)\n",
    "                if name == \"Manual_Trace\":\n",
    "                    tracker._attach_observation(run_id, root_span)\n",
    "                    continue\n",
    "\n",
    "                step_count += 1\n",
    "                print(f\"[CHAIN START] {name} (run_id: {run_id[:8]}...)\")\n",
    "\n",
    "                tracker.on_chain_start(\n",
    "                    run_id=run_id,\n",
    "                    name=name,\n",
    "                    inputs=data.get(\"input\"),\n",
    "                    parent_run_id=parent_run_id,\n",
    "                    metadata=metadata,\n",
    "                    tags=tags,\n",
    "                )\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_CHAIN_END\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_chain_end\":\n",
    "                if name == \"Manual_Trace\":\n",
    "                    final_output = data.get(\"output\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"[CHAIN END] {name}\")\n",
    "\n",
    "                tracker.on_chain_end(\n",
    "                    run_id=run_id,\n",
    "                    outputs=data.get(\"output\"),\n",
    "                )\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_CHAIN_ERROR\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_chain_error\":\n",
    "                print(f\"[CHAIN ERROR] {name}\")\n",
    "\n",
    "                error = data.get(\"error\", Exception(\"Unknown error\"))\n",
    "                tracker.on_chain_error(run_id=run_id, error=error)\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_CHAT_MODEL_START\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_chat_model_start\":\n",
    "                print(f\"[LLM START] {name}\")\n",
    "\n",
    "                # Extract model info from metadata\n",
    "                model_name = metadata.get(\"ls_model_name\", \"unknown\")\n",
    "\n",
    "                # Extract messages\n",
    "                messages = data.get(\"messages\", data.get(\"input\", []))\n",
    "\n",
    "                tracker.on_chat_model_start(\n",
    "                    run_id=run_id,\n",
    "                    name=name,\n",
    "                    messages=messages,\n",
    "                    parent_run_id=parent_run_id,\n",
    "                    model_name=model_name,\n",
    "                    metadata=metadata,\n",
    "                )\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_CHAT_MODEL_STREAM\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_chat_model_stream\":\n",
    "                # Update completion start time on first token\n",
    "                tracker.on_llm_new_token(run_id)\n",
    "\n",
    "                chunk = data.get(\"chunk\")\n",
    "                if chunk and hasattr(chunk, \"content\") and chunk.content:\n",
    "                    print(f\"[LLM STREAM] {chunk.content[:50]}...\")\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_CHAT_MODEL_END\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_chat_model_end\":\n",
    "                print(f\"[LLM END] {name}\")\n",
    "\n",
    "                output = data.get(\"output\")\n",
    "\n",
    "                # Parse usage\n",
    "                usage = _parse_usage_from_response(output)\n",
    "\n",
    "                # Format output\n",
    "                formatted_output = _format_llm_output(output)\n",
    "\n",
    "                # Extract model from response if available\n",
    "                model = None\n",
    "                if output and hasattr(output, \"response_metadata\"):\n",
    "                    model = output.response_metadata.get(\"model_name\")\n",
    "\n",
    "                tracker.on_chat_model_end(\n",
    "                    run_id=run_id,\n",
    "                    output=formatted_output,\n",
    "                    usage=usage,\n",
    "                    model=model,\n",
    "                )\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_LLM_ERROR\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_llm_error\":\n",
    "                print(f\"[LLM ERROR] {name}\")\n",
    "\n",
    "                error = data.get(\"error\", Exception(\"Unknown error\"))\n",
    "                obs, obs_context = tracker._detach_observation(run_id)\n",
    "                if obs is not None:\n",
    "                    obs.update(level=\"ERROR\", status_message=str(error))\n",
    "                    if obs_context:\n",
    "                        obs_context.__exit__(None, None, None)\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_TOOL_START\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_tool_start\":\n",
    "                print(f\"[TOOL START] {name}\")\n",
    "\n",
    "                tracker.on_tool_start(\n",
    "                    run_id=run_id,\n",
    "                    name=name,\n",
    "                    input_str=data.get(\"input\"),\n",
    "                    parent_run_id=parent_run_id,\n",
    "                    metadata=metadata,\n",
    "                )\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_TOOL_END\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_tool_end\":\n",
    "                print(f\"[TOOL END] {name}\")\n",
    "\n",
    "                tracker.on_tool_end(\n",
    "                    run_id=run_id,\n",
    "                    output=data.get(\"output\"),\n",
    "                )\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_TOOL_ERROR\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_tool_error\":\n",
    "                print(f\"[TOOL ERROR] {name}\")\n",
    "\n",
    "                error = data.get(\"error\", Exception(\"Unknown error\"))\n",
    "                tracker.on_tool_error(run_id=run_id, error=error)\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_RETRIEVER_START\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_retriever_start\":\n",
    "                print(f\"[RETRIEVER START] {name}\")\n",
    "\n",
    "                obs_context = langfuse_client.start_as_current_observation(\n",
    "                    as_type=\"retriever\",\n",
    "                    name=name,\n",
    "                    input=data.get(\"input\"),\n",
    "                    metadata=metadata,\n",
    "                )\n",
    "                obs = obs_context.__enter__()\n",
    "                tracker._attach_observation(run_id, obs, obs_context)\n",
    "\n",
    "            # =================================================================\n",
    "            # ON_RETRIEVER_END\n",
    "            # =================================================================\n",
    "            elif event_type == \"on_retriever_end\":\n",
    "                print(f\"[RETRIEVER END] {name}\")\n",
    "\n",
    "                obs, obs_context = tracker._detach_observation(run_id)\n",
    "                if obs is not None:\n",
    "                    obs.update(output=data.get(\"output\"))\n",
    "                    if obs_context:\n",
    "                        obs_context.__exit__(None, None, None)\n",
    "\n",
    "        # =====================================================================\n",
    "        # UPDATE ROOT SPAN WITH FINAL OUTPUT\n",
    "        # =====================================================================\n",
    "        root_span.update(output=final_output)\n",
    "\n",
    "        # =====================================================================\n",
    "        # CLEANUP\n",
    "        # =====================================================================\n",
    "        tracker.cleanup()\n",
    "\n",
    "        # Extract final response\n",
    "        if final_output and \"messages\" in final_output:\n",
    "            final_message = final_output[\"messages\"][-1]\n",
    "            final_response = final_message.content if hasattr(final_message, \"content\") else str(final_message)\n",
    "        else:\n",
    "            final_response = str(final_output)\n",
    "\n",
    "    # =========================================================================\n",
    "    # FLUSH: Ensure all observations are sent to Langfuse\n",
    "    # =========================================================================\n",
    "    langfuse_client.flush()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(f\"Final response: {final_response}\")\n",
    "    print(f\"Total chain steps: {step_count}\")\n",
    "    print(f\"Trace ID: {tracker.last_trace_id}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    return {\"session_id\": session_id, \"output\": final_response, \"trace_id\": tracker.last_trace_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Langfuse trace in Langfuse UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the weather in Miami?\"\n",
    "langfuse_client = get_client()\n",
    "asyncio.run(test_event_generator(graph=graph,user_input=user_input))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'session_id': 'b0ca3494-ad2a-404b-9de2-4f3237a0ec3e',\n",
    " 'output': 'None',\n",
    " 'trace_id': '1886704a3c27c30d92a63d62612c3616'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![manual_trace](langgraph_manual_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Send a Trace to langfuse automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CallbackHandler for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.langchain import CallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the weather in Miami?\"\n",
    "config = {\"callbacks\": [langfuse_handler]}\n",
    "input_state = State(messages=[user_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke(input_state, config=config)\n",
    "print(result.get('messages')[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Langfuse automatic trace in Langfuse UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![automatic_trace](langgraph_automatic_trace.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
