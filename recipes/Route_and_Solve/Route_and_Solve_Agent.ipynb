{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granite Route and Solve Agent\n",
    "\n",
    "In this recipe, you will use the IBM [Granite](https://www.ibm.com/granite) model now available on watsonx.ai to build a Route and Solve Agent. This agent extends the [Function Calling Agent](../Function_Calling/Function_Calling_Agent.ipynb) recipe by introducing a router node that intelligently distributes queries to specialized subagents, each with their own grouped set of tools.\n",
    "\n",
    "The Route and Solve architecture is ideal when you have a large number of tools that can be naturally grouped by category or domain. Instead of presenting all tools to a single agent, the router first determines which category of tools are needed, then routes the query to the appropriate subagent.\n",
    "\n",
    "This approach offers several benefits:\n",
    "- **Reduced tool selection errors**: Subagents only see relevant tools for their domain\n",
    "- **Better scalability**: Easy to add new tool categories as subagents\n",
    "- **Improved reasoning**: Specialized subagents can reason more effectively within their domain\n",
    "- **Clear separation of concerns**: Tools are logically grouped by functionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this recipe is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations.\n",
    "\n",
    "You can run this notebook in [Colab](https://colab.research.google.com/), or download it to your system and [run the notebook locally](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started_with_Jupyter_Locally/Getting_Started_with_Jupyter_Locally.md).\n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Note, this notebook is compatible with Python 3.12 and well as Python 3.11, the default in Colab at the time of publishing this recipe. To check your python version, you can run the `!python --version` command in a code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set up a watsonx.ai instance\n",
    "\n",
    "See [Getting Started with IBM watsonx](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_WatsonX.ipynb) for information on getting ready to use watsonx.ai.\n",
    "\n",
    "You will need three credentials from the watsonx.ai set up to add to your environment: `WATSONX_URL`, `WATSONX_APIKEY`, and `WATSONX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Install relevant libraries and set up credentials and the Granite model\n",
    "\n",
    "We'll need a few libraries for this recipe. We will be using LangGraph and LangChain libraries to use Granite on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    langgraph \\\n",
    "    langchain \\\n",
    "    langchain_ibm\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the credentials to use watsonx.ai and create the Granite model for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = \"ibm/granite-4-h-small\"\n",
    "\n",
    "llm_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "# --- 1. LLM Initialization (Agent's Brain) ---\n",
    "llm = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=llm_params,\n",
    ")\n",
    "print(f\"LLM initialized: {model}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
