{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolRag Agent with LangChain, Granite and watsonx.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to build a Retrieval-Augmented Generation (RAG) agent with a large, semantically-searchable toolset, powered by LangChain’s agent framework, IBM’s Granite LLM, and watsonx embeddings. You’ll see credential setup, tool semantic indexing, and agent orchestration for robust research and engineering workflows.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+ environment (e.g., Jupyter, Colab, or watsonx.ai).\n",
    "- Libraries: langgraph, langgraph-bigtool, langchain-ibm, langchain-huggingface, transformers, torch, python-dotenv.\n",
    "- IBM watsonx.ai credentials (API key, project ID) for Granite model access. Alternatively, use local Granite via transformers.\n",
    "\n",
    "Let's build a scalable ToolRag Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this recipe is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations. \n",
    "\n",
    "You can run this notebook in [Colab](https://colab.research.google.com/), or download it to your system and [run the notebook locally](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started_with_Jupyter_Locally/Getting_Started_with_Jupyter_Locally.md). \n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Note, this notebook is compatible with Python 3.12 and well as Python 3.11, the default in Colab at the time of publishing this recipe. To check your python version, you can run the `!python --version` command in a code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set up a watsonx.ai instance\n",
    "\n",
    "See [Getting Started with IBM watsonx](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_WatsonX.ipynb) for information on getting ready to use watsonx.ai. \n",
    "\n",
    "You will need three credentials from the watsonx.ai set up to add to your environment: `WATSONX_URL`, `WATSONX_APIKEY`, and `WATSONX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Install relevant libraries and set up credentials and the Granite model\n",
    "\n",
    "We'll need a few libraries for this recipe. We will be using LangGraph and LangChain libraries to use Granite on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "%pip install -qU langchain-ibm langgraph langgraph-bigtool ibm-watsonx-ai\n",
    "\n",
    "# Install RAG components (Vector Store and utilities)\n",
    "%pip install -q chromadb langchain-community langchain-chroma\n",
    "\n",
    "# Install IBM specific utility for easy credentials load\n",
    "%pip install -q \"git+https://github.com/ibm-granite-community/utils.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication and model initialization \n",
    "\n",
    "The next step involves initialization of the watsonx LLM (used for the agent's reasoning) and watsonx Embeddings (for Tool-RAG semantic search).\n",
    "\n",
    "**Note:** Ensure your environment variables (`WATSONX_URL`, `WATSONX_APIKEY`, `WATSONX_PROJECT_ID`) are set before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import types\n",
    "import uuid\n",
    "from getpass import getpass\n",
    "from typing_extensions import Annotated\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langgraph_bigtool import create_agent\n",
    "from langgraph.store.memory import InMemoryStore as LangGraphStore\n",
    "from langchain_core.messages import HumanMessage\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- Configuration ---\n",
    "model = \"ibm/granite-3-3-8b-instruct\"\n",
    "\n",
    "llm_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "# --- 1. LLM Initialization (Agent's Brain) ---\n",
    "llm = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=llm_params,\n",
    ")\n",
    "print(f\"LLM initialized: {model}\")\n",
    "\n",
    "\n",
    "# --- 2. Embeddings Initialization (Tool-RAG Indexer) ---\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-107m-multilingual\",\n",
    "    url=get_env_var(\"WATSONX_URL\"),\n",
    "    apikey=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3\n",
    "    }\n",
    ")\n",
    "print(\"Embeddings initialized: ibm/granite-embedding-107m-multilingual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Tools (The \"Big Tool\" Set)\n",
    "\n",
    "The use of the Python `math` module serves as an ideal \"Big Tool\" set for this demonstration. To demonstrate the \"Big Tool\" concept—where an agent must select a few relevant tools from a very large set—we will use the entire public API of Python's built-in math module. The module contains roughly 50 functions (including logarithmic, trigonometric, hyperbolic, power, and number-theoretic functions), providing a sufficiently large toolset to stress the RAG mechanism\n",
    "\n",
    "In the next step, we will iterate over every function in the math module, convert them into `langchain_core.tools.Tool` objects, and store their descriptions in a Chroma vector store. This will create our indexed \"Big Tool\" registry, enabling the RAG-Tooling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph_bigtool.utils import convert_positional_only_function_to_tool\n",
    "\n",
    "# 1. Collect all functions from `math` as an example set of \"Big Tools\"\n",
    "all_tools = []\n",
    "for function_name in dir(math):\n",
    "    function = getattr(math, function_name)\n",
    "    if not isinstance(function, types.BuiltinFunctionType):\n",
    "        continue\n",
    "    # Convert functions to LangChain tools\n",
    "    lc_tool = convert_positional_only_function_to_tool(function)\n",
    "    # FIX: Only include tools that were successfully converted (i.e., not None)\n",
    "    if lc_tool is not None:\n",
    "        all_tools.append(lc_tool)\n",
    "\n",
    "print(f\"Total tools collected and successfully converted: {len(all_tools)}\")\n",
    "\n",
    "# 2. Create the tool registry dictionary (id -> tool) for BigTool\n",
    "tool_registry = {}\n",
    "for t in all_tools:\n",
    "    # Assign a unique ID to each tool\n",
    "    tool_registry[str(uuid.uuid4())] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Tool-RAG Recipe (Custom Retrieval Function)\n",
    "\n",
    "This section implements the **Tool-RAG** mechanism. We index tool descriptions into a **Chroma** vector store using **watsonx Embeddings**, and define a custom function to retrieve the most relevant tools based on the user query.\n",
    "\n",
    "The Tool RAG (Retrieval-Augmented Generation) uses the concepts of Tool calling and RAG:\n",
    "\n",
    "1. Tool Indexing (The RAG Step): All tool metadata (names, descriptions, and schemas) are treated as documents and embedded into a Vector Store (in our case, ChromaDB).\n",
    "2. Tool Retrieval: When a user asks a question, a retrieval step is executed first. The user's query is used to perform a semantic search against the Vector Store.\n",
    "3. Dynamic Binding: Only the top K most semantically relevant tools are retrieved and dynamically bound to the LLM's prompt. This keeps the prompt concise and relevant.\n",
    "4. Tool Execution: The LLM will only see a handful of relevant tools, where it can reliably select and call the correct one.\n",
    "\n",
    "This approach ensures the agent can scale to a virtually unlimited number of tools while maintaining high performance and prompt efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "TOOL_ID_KEY = \"\\u200btool_id\"\n",
    "\n",
    "# 1. Initialize the Chroma Vector Store\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=watsonx_embedding,\n",
    "    collection_name=\"tool_rag_index_watsonx_v4_clean\",\n",
    ")\n",
    "\n",
    "# Ensure the collection is ready. Use reset_collection to ensure a clean, initialized collection.\n",
    "try:\n",
    "    vectorstore.reset_collection()\n",
    "except Exception as e:\n",
    "    print(f\"Warning during collection reset: {e}\")\n",
    "    pass\n",
    "\n",
    "# Index tools into Chroma\n",
    "tool_documents = []\n",
    "for tool_id, t in tool_registry.items():\n",
    "    tool_documents.append(\n",
    "        Document(\n",
    "            # The page_content is what gets embedded for RAG search\n",
    "            page_content=f\"{t.name}: {t.description}\",\n",
    "            metadata={TOOL_ID_KEY: tool_id}\n",
    "        )\n",
    "    )\n",
    "\n",
    "doc_ids = vectorstore.add_documents(tool_documents)\n",
    "print(f\"Successfully indexed {len(doc_ids)} tool descriptions in Chroma.\")\n",
    "\n",
    "\n",
    "# 2. Define the Custom Tool Retrieval Function (The RAG Logic)\n",
    "def retrieve_tools_from_chroma(query: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve tool IDs from the Chroma vector store based on the user's query.\n",
    "    This function implements the 'Tool-RAG' step.\n",
    "    \"\"\"\n",
    "    # Use Chroma's similarity search to find the top 2 most relevant tool descriptions\n",
    "    results = vectorstore.similarity_search(query, k=2)\n",
    "    \n",
    "    # Extract the original tool_id from the metadata\n",
    "    tool_ids = [doc.metadata[TOOL_ID_KEY] for doc in results]\n",
    "    return tool_ids\n",
    "\n",
    "print(\"Custom tool retrieval function defined, connecting BigTool to Chroma/watsonx RAG.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building and Compiling the ToolRag Agent\n",
    "\n",
    "This cell constructs the core logic of our agent using LangGraph, the state machine layer for LangChain. LangGraph allows us to define the specific steps and decision points in our agent's workflow. We use `langgraph_bigtool.create_agent` and pass the custom RAG function to build the final agent graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph_bigtool import create_agent\n",
    "from langgraph.store.memory import InMemoryStore as LangGraphStore\n",
    "\n",
    "# Create the BigTool Agent Builder, passing the custom retrieval function\n",
    "builder = create_agent(\n",
    "    llm, \n",
    "    tool_registry,\n",
    "    # This plugs the custom RAG logic into the agent's workflow\n",
    "    retrieve_tools_function=retrieve_tools_from_chroma \n",
    ")\n",
    "\n",
    "# Compile the graph. We use the standard `LangGraphStore` for graph state persistence.\n",
    "agent = builder.compile(store=LangGraphStore()) \n",
    "\n",
    "print(\"ToolRag Agent (BigTool + watsonx + Chroma RAG) compiled successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
