{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolRAG Agent with LangChain, Granite and watsonx.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToolRAG is a method that helps AI systems become more powerful and useful by combining two key abilities:\n",
    "\n",
    "- Retrieval-Augmented Generation [RAG](https://research.ibm.com/blog/retrieval-augmented-generation-RAG): This means the AI can look up relevant information from a database or document store before answering.\n",
    "- Tool use: The AI can choose and use tools (like calculators, search engines, APIs, or code execution) to solve problems.\n",
    "\n",
    "This approach is part of a larger series on [Agent Architectures](https://github.com/ibm-granite-community/granite-agent-cookbook/blob/main/building_agents.md), which explores how to take agents from prototype to production. ToolRAG is an architecture designed for agents that need to use a large set of tools. Instead of just guessing or generating text, the AI can look things up and take action. ToolRAG is an architecture designed for agents that need to use a large set of tools. It pre-filters the available tools from the LLM's main reasoning by first using a vector database to semantically retrieve the most relevant tools for a query. This prevents overloading the LLM's context window with hundreds of tool definitions, significantly improving efficiency and performance for tool-using agents.\n",
    "\n",
    "This notebook shows how to build a Retrieval-Augmented Generation (RAG) agent with a large, semantically-searchable toolset, powered by LangChain’s agent framework, IBM’s Granite LLM, and watsonx embeddings. You’ll see setup of the credentials, tool semantic indexing, and agent orchestration for robust research and engineering workflows. Let's build a scalable ToolRAG Agent!\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+ environment (e.g., Jupyter, Colab, or watsonx.ai)\n",
    "- IBM watsonx.ai credentials (API key, project ID) for Granite model access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this recipe is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations. \n",
    "\n",
    "You can run this notebook in [Colab](https://colab.research.google.com/), or download it to your system and [run the notebook locally](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started_with_Jupyter_Locally/Getting_Started_with_Jupyter_Locally.md). \n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Note, this notebook is compatible with Python 3.12 and well as Python 3.11, the default in Colab at the time of publishing this recipe. To check your python version, you can run the `!python --version` command in a code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set up a watsonx.ai instance\n",
    "\n",
    "See [Getting Started with IBM watsonx](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_WatsonX.ipynb) for information on getting ready to use watsonx.ai. \n",
    "\n",
    "You will need three credentials from the watsonx.ai set up to add to your environment: `WATSONX_URL`, `WATSONX_APIKEY`, and `WATSONX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Install relevant libraries and set up credentials and the Granite model\n",
    "\n",
    "We'll need a few libraries for this recipe. We will be using LangGraph and LangChain libraries to use Granite on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "%pip install -qU langchain langchain-ibm langchain-core langgraph grandalf\n",
    "\n",
    "# Install RAG components (Vector Store and utilities)\n",
    "%pip install -q chromadb langchain-chroma\n",
    "\n",
    "# Install IBM specific utility for easy credentials load\n",
    "%pip install -q ibm-watsonx-ai \"git+https://github.com/ibm-granite-community/utils.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Authentication and model initialization \n",
    "\n",
    "The next step involves initialization of the watsonx LLM (used for the agent's reasoning) and watsonx Embeddings (for Tool-RAG semantic search).\n",
    "\n",
    "**Note:** Ensure your environment variables (`WATSONX_URL`, `WATSONX_APIKEY`, `WATSONX_PROJECT_ID`) are set before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import operator\n",
    "from getpass import getpass\n",
    "from typing import List, Dict, Any\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
    "from langchain_core.tools import StructuredTool, BaseTool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, BaseMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_chroma import Chroma\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain.chat_models import init_chat_model\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "\n",
    "# --- Configuration ---\n",
    "model = \"ibm/granite-3-3-8b-instruct\"\n",
    "\n",
    "llm_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "# --- 1. LLM Initialization (Agent's Brain) ---\n",
    "llm = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=llm_params,\n",
    ")\n",
    "print(f\"LLM initialized: {model}\")\n",
    "\n",
    "\n",
    "# --- 2. Embeddings Initialization (Tool-RAG Indexer) ---\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "    url=get_env_var(\"WATSONX_URL\"),\n",
    "    apikey=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3\n",
    "    }\n",
    ")\n",
    "print(\"Embeddings initialized: ibm/granite-embedding-278m-multilingual\")\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose `ibm/granite-embedding-278m-multilingual` because it is an IBM Granite family model, ensuring seamless integration with other watsonx components.\n",
    "\n",
    "Its multilingual capability is a benefit for real-world scenarios, and the '278m' size provides a good balance between performance and embedding quality for semantic retrieval of tool descriptions. We use ChromaDB as the underlying vector store for tool metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define small tools and data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ToolRAG concept are specifically designed to address the scalability problem that arises when an agent has hundreds or thousands of tools. In classic \"tool-calling\" architectures, the definitions of all tools must be inserted directly into the LLM's prompt, which quickly hits the context window limit and degrades performance.\n",
    "\n",
    "For this Proof of Concept (PoC), we use a small set of 5 tools for the following reasons:\n",
    "- Isolation of ToolRAG mechanics: By keeping the total tool count small, we can easily verify that the core ToolRAG retrieval mechanism is working correctly. When a query is given, we can visually confirm that the vector store is correctly selecting the top K=3 most relevant tool definitions (e.g., finance tools for a financial query), even though the LLM could technically handle all 5 tools without RAG.\n",
    "- Focus on selection logic, and avoid context overflow: This setup allows us to focus on the agent's ability to select a filtered subset of tools (`get_weather_forecast`, `get_stock_price`, `convert_currency`) from the small indexed pool, and then correctly chain their execution, without introducing the complexity of context window management or large-scale tool embedding.\n",
    "- Simulating a scalable environment: In a production setting, this small set of 5 tools would ideally be hundreds of other tools (e.g., 50 financial, 50 IT, 50 HR tools). The retrieval mechanism shown here is what scales to those environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_future_value(principal: float, rate: float, years: int) -> str:\n",
    "    \"\"\"Calculates the future value of an investment using compound interest.\"\"\"\n",
    "    future_value = principal * ((1 + rate) ** years)\n",
    "    return f\"The future value is ${future_value:.2f}.\"\n",
    "\n",
    "def get_stock_price(ticker: str) -> str:\n",
    "    \"\"\"Fetches the current or historical stock price for a given ticker symbol (e.g., IBM).\"\"\"\n",
    "    if ticker == \"IBM\":\n",
    "        return \"The current price for IBM is $313.72.\"\n",
    "    return f\"Stock price for {ticker} not found in this mock-up.\"\n",
    "\n",
    "def check_developer_skill(skill: str) -> str:\n",
    "    \"\"\"Checks the availability of an AI developer with a specific programming or ML skill.\"\"\"\n",
    "    if 'python' in skill.lower() or 'langchain' in skill.lower():\n",
    "        return \"Yes, we have multiple experienced developers with that skill set.\"\n",
    "    return \"Developer with that specific skill is currently limited.\"\n",
    "\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Converts a monetary amount between two specified currencies (e.g., USD to EUR).\"\"\"\n",
    "    if from_currency == \"USD\" and to_currency == \"EUR\":\n",
    "        converted = amount * 0.92\n",
    "        return f\"{amount} USD is approximately {converted:.2f} EUR.\"\n",
    "    return f\"Conversion from {from_currency} to {to_currency} is not supported.\"\n",
    "\n",
    "def get_weather_forecast(city: str) -> str:\n",
    "    \"\"\"Provides the current weather forecast for a specified city.\"\"\"\n",
    "    if 'boston' in city.lower():\n",
    "        return \"Boston's current weather is partly cloudy with a temperature of 5°C.\"\n",
    "    return f\"Weather data for {city} is not available in this mock-up.\"\n",
    "\n",
    "functions = [calculate_future_value, get_stock_price, check_developer_skill, convert_currency, get_weather_forecast]\n",
    "\n",
    "# Wrap into StructuredTool\n",
    "tools: List[BaseTool] = []\n",
    "for func in functions:\n",
    "    tool = StructuredTool.from_function(\n",
    "        func=func,\n",
    "        name=func.__name__, \n",
    "        description=func.__doc__.strip()\n",
    "    )\n",
    "    tools.append(tool)\n",
    "\n",
    "print(\"=== All Initialized Tools ===\")\n",
    "for i, tool in enumerate(tools):\n",
    "    print(f\"  Tool {i+1}: '{tool.name}'\")\n",
    "print(f\"\\nTotal tools initialized: {len(tools)}\")\n",
    "print(\"All tools ready for indexing and agent use!\\n\")\n",
    "\n",
    "# tool_map for ToolNode\n",
    "tool_map = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create tool registry and index tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before our agent can perform **Tool-RAG** (Tool Retrieval-Augmented Generation), we need to create a searchable index of our tools. This involves:\n",
    "\n",
    "- **Extracting Tool Metadata**: Convert each tool's description (natural language summary) into a `Document` object, tagged with its name for later lookup.\n",
    "- **Embedding & Storing**: Use watsonx embeddings to vectorize descriptions, then persist them in Chroma. This enables semantic similarity searches (e.g., \"weather-related tools\" matches `get_weather_forecast` via cosine similarity on embeddings).\n",
    "- **Retriever Setup**: Configure a retriever with MMR (Maximal Marginal Relevance) search to fetch diverse, relevant tools – avoiding redundant results (e.g., two finance tools for a stock query).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Generate Tool Documents for Indexing (List Comprehension)\n",
    "\n",
    "tool_docs = [\n",
    "    Document(page_content=tool.description, metadata={\"tool_name\": tool.name})\n",
    "    for tool in tools\n",
    "]\n",
    "\n",
    "# Step 2: Ingest into Chroma Vector Store\n",
    "vectorstore = Chroma.from_documents(documents=tool_docs, embedding=watsonx_embedding)\n",
    "\n",
    "# Step 3: Create Configured Retriever for Graph Integration\n",
    "tool_retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Indexed {len(tool_docs)} tools in vectorstore.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Agent capabilities: tool selection and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we construct a **vanilla LangGraph agent** and this \"from-scratch\" approach demonstrates the core concepts below: \n",
    "\n",
    "- **State Management**: Using `TypedDict` for thread-safe, annotated state (messages + retrieved tools).\n",
    "- **Graph Structure**: Nodes for retrieval, reasoning (LLM), and action (tools). The edges are added for flow control.\n",
    "- **Tool-RAG Pattern**: Semantically retrieve a subset of tools *before* LLM invocation to handle large toolsets efficiently.\n",
    "\n",
    "**High-Level Flow**:\n",
    "1. **Retrieve Tools Node**: Uses ToolRAG to select relevant tools based on query semantics.\n",
    "2. **LLM Node**: Dynamically binds *only* retrieved tools to the LLM (reduces context bloat).\n",
    "3. **Conditional Edge**: Routes to tools if calls detected, else END.\n",
    "4. **Tool Node**: Executes calls using `ToolNode`.\n",
    "5. **Loop Back**: From tools to LLM for multi-turn reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Custom tool retrieval function\n",
    "\n",
    "This retrieval function Bridges vectorstore (semantic search) to agent state (tool objects). It performs the following steps:\n",
    "\n",
    "- Semantic Search – Query vectorstore for similar tool descriptions. Uses `.similarity_search_with_score`: Returns (doc, score) pairs; lower score = more relevant.\n",
    "\n",
    "- Map Docs to tools – Extract metadata['tool_name'], lookup in full tools list. We perform the lookup because vectorstore holds docs (text only); and we need callable `BaseTools` for invocation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def custom_retrieve_tools(query: str, limit: int = 5) -> List[BaseTool]:\n",
    "    results = vectorstore.similarity_search_with_score(query, k=limit)\n",
    "    tool_map_lookup = {tool.name: tool for tool in tools}\n",
    "    retrieved_tools = [\n",
    "        tool_map_lookup[doc.metadata['tool_name']] \n",
    "        for doc, _ in results \n",
    "        if doc.metadata['tool_name'] in tool_map_lookup\n",
    "    ]\n",
    "    print(f\"[ToolRAG]: Retrieved {len(retrieved_tools)}: {[t.name for t in retrieved_tools]}\")\n",
    "    return retrieved_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create the ToolRAG Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we dive into the **ToolRAG pattern** (Tool Retrieval-Augmented Generation): Semantically retrieving a subset of tools before LLM binding/execution. \n",
    "\n",
    "This enables multi-tool queries (e.g., \"Weather + Stock + Convert\") by chaining retrieval with tool calls plus reasoning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define the Agent State Schema\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    retrieved_tools: List[BaseTool]\n",
    "print(\"Step 1: AgentState schema defined – Tracks messages and retrieved tools for graph flow.\")\n",
    "\n",
    "# Step 2: Define Retrieval Node (ToolRAG Integration)\n",
    "def retrieve_tools_node(state: AgentState) -> AgentState:\n",
    "    query = state[\"messages\"][-1].content\n",
    "    retrieved_tools = custom_retrieve_tools(query, limit=3)\n",
    "    return {\"messages\": state[\"messages\"], \"retrieved_tools\": retrieved_tools}\n",
    "\n",
    "print(\"Step 2: retrieve_tools_node defined – Implements ToolRAG as first graph step after START.\")\n",
    "\n",
    "# Step 3: Define LLM Reasoning Node (Dynamic Tool Binding)\n",
    "def llm_node(state: AgentState) -> AgentState:\n",
    "    retrieved_tools = state[\"retrieved_tools\"]\n",
    "    bound_llm = llm.bind_tools(retrieved_tools)\n",
    "    response = bound_llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response], \"retrieved_tools\": retrieved_tools}\n",
    "    \n",
    "print(\"Step 3: llm_node defined – Dynamically binds ToolRAG tools to LLM for efficient reasoning.\")\n",
    "\n",
    "# Step 4: Define Tool Execution Node (Using Prebuilt ToolNode)\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "print(\"Step 4: tool_node created – Prebuilt executor for parallel tool calls.\")\n",
    "\n",
    "# Step 5: Define Conditional Routing (should_continue)\n",
    "def should_continue(state: AgentState):\n",
    "    return \"tools\" if state['messages'][-1].tool_calls else END\n",
    "\n",
    "print(\"Step 5: should_continue defined – Routes graph flow based on tool calls.\")\n",
    "\n",
    "# Step 6: Assemble & Compile the Graph\n",
    "\n",
    "workflow = StateGraph(state_schema=AgentState)\n",
    "workflow.add_node(\"retrieve_tools\", retrieve_tools_node)\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve_tools\")\n",
    "\n",
    "workflow.add_edge(\"retrieve_tools\", \"llm\")\n",
    "workflow.add_conditional_edges(\"llm\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "workflow.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "# Step 7: Compile and return a runnable graph\n",
    "graph = workflow.compile()\n",
    "print(\"Step 6: Graph assembled & ready. Tool-RAG Agent compiled successfully!\")\n",
    "\n",
    "print(f\"\\n=== LangGraph ASCII Visualization ===\")\n",
    "\n",
    "graph.get_graph().print_ascii()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Testing the agent with a query requiring 3 tools or a subset of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "user_query = \"What's the weather like in Boston, and can you check the stock price for IBM, then convert 100 USD to EUR?\"\n",
    "final_result = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(content=user_query)], \"custom_retrieve_tools\": []},  # Init empty\n",
    "    config=config\n",
    ")\n",
    "\n",
    "final_message = final_result[\"messages\"][-1]\n",
    "print(\"\\n--- Final Agent Response ---\")\n",
    "print(final_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Leveraging pre-built libraries for ToolRAG agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We built a Tool-RAG agent from scratch using vanilla LangGraph, however there are couple of **pre-built libraries** that abstract the boilerplate (e.g., indexing, retrieval, dynamic binding). These accelerate development for production-scale agents with loads of available tools, while preserving the core pattern of semantic retrieval plus subset binding to execution.\n",
    "\n",
    "Key Pre-Built Libraries for Tool-RAG\n",
    "- [langgraph-bigtool](https://github.com/langchain-ai/langgraph-bigtool): LangGraph extension for \"Big Tool\" agents; automates Tool-RAG with registry-based retrieval\n",
    "- [LangChain Toolkits](https://docs.langchain.com/oss/javascript/integrations/tools) (e.g., langchain-agents): Modular agent builders with RAG-infused toolkits like create_react_agent\n",
    "- [LlamaIndex Tool Integration](https://developers.llamaindex.ai/python/framework/module_guides/deploying/agents/tools/): Index-based RAG for tools via VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
