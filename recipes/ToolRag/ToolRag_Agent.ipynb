{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolRAG Agent with LangChain, Granite and watsonx.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToolRAG is a method that helps AI systems become more powerful and useful by combining two key abilities:\n",
    "\n",
    "- Retrieval-Augmented Generation [RAG](https://research.ibm.com/blog/retrieval-augmented-generation-RAG): This means the AI can look up relevant information from a database or document store before answering.\n",
    "- Tool use: The AI can choose and use tools (like calculators, search engines, APIs, or code execution) to solve problems.\n",
    "\n",
    "This approach is part of a larger series on [Agent Architectures](https://github.com/ibm-granite-community/granite-agent-cookbook/blob/main/building_agents.md), which explores how to take agents from prototype to production. ToolRAG is an architecture designed for agents that need to use a large set of tools.\n",
    "\n",
    "Instead of just guessing or generating text, the AI can look things up and take action. ToolRAG is an architecture designed for agents that need to use a large set of tools. It pre-filters the available tools from the LLM's main reasoning by first using a vector database to semantically retrieve the most relevant tools for a query. This prevents overloading the LLM's context window with hundreds of tool definitions, significantly improving efficiency and performance for tool-using agents.\n",
    "\n",
    "This notebook shows how to build a Retrieval-Augmented Generation (RAG) agent with a large, semantically-searchable toolset, powered by LangChain’s agent framework, IBM’s Granite LLM, and watsonx embeddings. You’ll see setup of the credentials, tool semantic indexing, and agent orchestration for robust research and engineering workflows.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+ environment (e.g., Jupyter, Colab, or watsonx.ai)..\n",
    "- IBM watsonx.ai credentials (API key, project ID) for Granite model access.\n",
    "\n",
    "Let's build a scalable ToolRAG Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this recipe is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations. \n",
    "\n",
    "You can run this notebook in [Colab](https://colab.research.google.com/), or download it to your system and [run the notebook locally](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started_with_Jupyter_Locally/Getting_Started_with_Jupyter_Locally.md). \n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Note, this notebook is compatible with Python 3.12 and well as Python 3.11, the default in Colab at the time of publishing this recipe. To check your python version, you can run the `!python --version` command in a code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set up a watsonx.ai instance\n",
    "\n",
    "See [Getting Started with IBM watsonx](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_WatsonX.ipynb) for information on getting ready to use watsonx.ai. \n",
    "\n",
    "You will need three credentials from the watsonx.ai set up to add to your environment: `WATSONX_URL`, `WATSONX_APIKEY`, and `WATSONX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Install relevant libraries and set up credentials and the Granite model\n",
    "\n",
    "We'll need a few libraries for this recipe. We will be using LangGraph and LangChain libraries to use Granite on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "%pip install -qU langchain langchain-ibm langgraph toolregistry\n",
    "\n",
    "# Install RAG components (Vector Store and utilities)\n",
    "%pip install -q chromadb langchain-chroma\n",
    "\n",
    "# Install IBM specific utility for easy credentials load\n",
    "%pip install -q ibm-watsonx-ai \"git+https://github.com/ibm-granite-community/utils.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Authentication and model initialization \n",
    "\n",
    "The next step involves initialization of the watsonx LLM (used for the agent's reasoning) and watsonx Embeddings (for Tool-RAG semantic search).\n",
    "\n",
    "**Note:** Ensure your environment variables (`WATSONX_URL`, `WATSONX_APIKEY`, `WATSONX_PROJECT_ID`) are set before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from getpass import getpass\n",
    "from typing import List, Dict, Any\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from toolregistry import ToolRegistry\n",
    "\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain.chat_models import init_chat_model\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "\n",
    "# --- Configuration ---\n",
    "model = \"ibm/granite-3-3-8b-instruct\"\n",
    "\n",
    "llm_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "# --- 1. LLM Initialization (Agent's Brain) ---\n",
    "llm = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=llm_params,\n",
    ")\n",
    "print(f\"LLM initialized: {model}\")\n",
    "\n",
    "\n",
    "# --- 2. Embeddings Initialization (Tool-RAG Indexer) ---\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-107m-multilingual\",\n",
    "    url=get_env_var(\"WATSONX_URL\"),\n",
    "    apikey=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3\n",
    "    }\n",
    ")\n",
    "print(\"Embeddings initialized: ibm/granite-embedding-107m-multilingual\")\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose `ibm/granite-embedding-107m-multilingual` because it is an IBM Granite family model, ensuring seamless integration with other watsonx components.\n",
    "\n",
    "Its multilingual capability is a benefit for real-world scenarios, and the '107m' size provides a good balance between performance and embedding quality for semantic retrieval of tool descriptions. We use ChromaDB as the underlying vector store for tool metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define small tools and data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ToolRAG concept are specifically designed to address the scalability problem that arises when an agent has hundreds or thousands of tools. In classic \"tool-calling\" architectures, the definitions of all tools must be inserted directly into the LLM's prompt, which quickly hits the context window limit and degrades performance.\n",
    "\n",
    "For this Proof of Concept (PoC), we use a small set of 5 tools for the following reasons:\n",
    "- Isolation of ToolRAG mechanics: By keeping the total tool count small, we can easily verify that the core ToolRAG retrieval mechanism is working correctly. When a query is given, we can visually confirm that the vector store is correctly selecting the top K=3 most relevant tool definitions (e.g., finance tools for a financial query), even though the LLM could technically handle all 5 tools without RAG.\n",
    "- Focus on selection logic, and avoid context overflow: This setup allows us to focus on the agent's ability to select a filtered subset of tools (`get_weather_forecast`, `get_stock_price`, `convert_currency`) from the small indexed pool, and then correctly chain their execution, without introducing the complexity of context window management or large-scale tool embedding.\n",
    "- Simulating a scalable environment: In a production setting, this small set of 5 tools would ideally be hundreds of other tools (e.g., 50 financial, 50 IT, 50 HR tools). The retrieval mechanism shown here is what scales to those environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculate_future_value(principal: float, rate: float, years: int) -> str:\n",
    "    \"\"\"Calculates the future value of an investment using compound interest.\"\"\"\n",
    "    future_value = principal * ((1 + rate) ** years)\n",
    "    return f\"The future value is ${future_value:.2f}.\"\n",
    "\n",
    "@tool\n",
    "def get_stock_price(ticker: str) -> str:\n",
    "    \"\"\"Fetches the current or historical stock price for a given ticker symbol (e.g., IBM).\"\"\"\n",
    "    # Placeholder for a real API call\n",
    "    if ticker == \"IBM\":\n",
    "        return \"The current price for IBM is $185.50.\"\n",
    "    return f\"Stock price for {ticker} not found in this mock-up.\"\n",
    "\n",
    "@tool\n",
    "def check_developer_skill(skill: str) -> str:\n",
    "    \"\"\"Checks the availability of an AI developer with a specific programming or ML skill.\"\"\"\n",
    "    if 'python' in skill.lower() or 'langchain' in skill.lower():\n",
    "        return \"Yes, we have multiple experienced developers with that skill set.\"\n",
    "    return \"Developer with that specific skill is currently limited.\"\n",
    "\n",
    "@tool\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Converts a monetary amount between two specified currencies (e.g., USD to EUR).\"\"\"\n",
    "    # Placeholder for a real currency conversion\n",
    "    if from_currency == \"USD\" and to_currency == \"EUR\":\n",
    "        converted = amount * 0.92\n",
    "        return f\"{amount} USD is approximately {converted:.2f} EUR.\"\n",
    "    return f\"Conversion from {from_currency} to {to_currency} is not supported.\"\n",
    "\n",
    "@tool\n",
    "def get_weather_forecast(city: str) -> str:\n",
    "    \"\"\"Provides the current weather forecast for a specified city.\"\"\"\n",
    "    if 'boston' in city.lower():\n",
    "        return \"Boston's current weather is partly cloudy with a temperature of 15°C.\"\n",
    "    return f\"Weather data for {city} is not available in this mock-up.\"\n",
    "\n",
    "tools = [\n",
    "    calculate_future_value, \n",
    "    get_stock_price, \n",
    "    check_developer_skill, \n",
    "    convert_currency, \n",
    "    get_weather_forecast\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create tool registry and index tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the following steps here:\n",
    "\n",
    "- Initialize tool registry. This is the registry that holds the tools, but is now decoupled from the storage.\n",
    "- Register/index the tools into the Chroma vector store.\n",
    "- Get the documents/metadata from the tools\n",
    "- Add the tool documents to the vector store\n",
    "- Create the retriever for the BigTool agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the documents/metadata from the tools\n",
    "tool_docs = []\n",
    "for tool_obj in tools:\n",
    "    if isinstance(tool_obj, BaseTool):\n",
    "        tool_docs.append(\n",
    "            Document(\n",
    "                page_content=tool_obj.description,\n",
    "                metadata={\"tool_name\": tool_obj.name}\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Add the tool documents to the vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=tool_docs,\n",
    "    embedding=watsonx_embedding\n",
    ")\n",
    "\n",
    "# Create the retriever for Tool-RAG (used in graph node)\n",
    "tool_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 relevant tools\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "print(f\"Total tools available: {len(tools)}\")\n",
    "print(f\"Tools indexed in Chroma vectorstore: {len(tool_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Agent capabilities: tool selection and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def custom_retrieve_tools(query: str, limit: int = 5) -> list[BaseTool]:\n",
    "    \"\"\"Retrieves the top N tools most semantically similar to the query.\n",
    "    \n",
    "    Returns actual tool objects (not just names) for dynamic binding in llm_node.\n",
    "    \"\"\"\n",
    "    # Use standalone vectorstore (no registry dependency)\n",
    "    results = vectorstore.similarity_search_with_score(query, k=limit)\n",
    "    \n",
    "    # Get tool objects by name (from global 'tools' dict for lookup)\n",
    "    tool_map = {tool.name: tool for tool in tools}  # Simple lookup dict\n",
    "    retrieved_tools = [tool_map[result[0].metadata['tool_name']] for result in results if result[0].metadata['tool_name'] in tool_map]\n",
    "    \n",
    "    print(f\"\\n[ToolRAG Retrieval]: Found {len(retrieved_tools)} relevant tools based on query: {[t.name for t in retrieved_tools]}\")\n",
    "    return retrieved_tools  # Return tools, not just names—for bind_tools()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ToolRAG Agent\n",
    "\n",
    "The `create_agent` function handles the LangGraph state machine and RAG-tool fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph_bigtool import create_agent\n",
    "from typing import Mapping, Callable, Any\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "tool_map = { \n",
    "    name: tool_registry.get_callable(name)\n",
    "    for name in tool_registry._tools.keys() \n",
    "}\n",
    "\n",
    "store = InMemorySaver()\n",
    "\n",
    "agent_builder = create_agent(\n",
    "    llm=llm,\n",
    "    tool_registry=tool_map,\n",
    "    retrieve_tools_function=custom_retrieve_tools\n",
    ")\n",
    "\n",
    "agent = agent_builder.compile(checkpointer=store) \n",
    "print(\"ToolRAG Agent compiled successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the agent with a query requiring 3 tools or a subset of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": thread_id\n",
    "    }\n",
    "}\n",
    "\n",
    "user_query = \"What's the weather like in Boston, and can you check the stock price for IBM, then convert 100 USD to EUR?\"\n",
    "\n",
    "print(f\"--- Running Agent with Query: {user_query} ---\")\n",
    "\n",
    "# Run the agent, passing the new config\n",
    "final_result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=user_query)]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Extract and print the final response\n",
    "final_message = final_result[\"messages\"][-1]\n",
    "\n",
    "print(\"\\n--- Final Agent Response ---\")\n",
    "print(final_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
