{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolRag Agent with LangChain, Granite and watsonx.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToolRAG is a method that helps AI systems become more powerful and useful by combining two key abilities:\n",
    "\n",
    "- Retrieval-Augmented Generation (RAG): This means the AI can look up relevant information from a database or document store before answering.\n",
    "- Tool use: The AI can choose and use tools (like calculators, search engines, APIs, or code execution) to solve problems.\n",
    "\n",
    "ToolRAG combines these two abilities in a smart way:\n",
    "\n",
    "- The AI retrieves information that helps it understand the problem better.\n",
    "- Then it decides which tool to use based on that information.\n",
    "- Finally, it uses the tool to generate a more accurate or useful response.\n",
    "\n",
    "So instead of just guessing or generating text, the AI can look things up and take action. ToolRag is an architecture designed for agents that need to use a large set of tools. It decouples tool selection from the LLM's main reasoning by first using a vector database to semantically retrieve the most relevant tools for a query. This prevents overloading the LLM's context window with hundreds of tool definitions, significantly improving efficiency and performance for tool-using agents.\n",
    "\n",
    "This notebook shows how to build a Retrieval-Augmented Generation (RAG) agent with a large, semantically-searchable toolset, powered by LangChain’s agent framework, IBM’s Granite LLM, and watsonx embeddings. You’ll see setup of the credentials, tool semantic indexing, and agent orchestration for robust research and engineering workflows.\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+ environment (e.g., Jupyter, Colab, or watsonx.ai)..\n",
    "- IBM watsonx.ai credentials (API key, project ID) for Granite model access.\n",
    "\n",
    "Let's build a scalable ToolRag Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this recipe is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations. \n",
    "\n",
    "You can run this notebook in [Colab](https://colab.research.google.com/), or download it to your system and [run the notebook locally](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started_with_Jupyter_Locally/Getting_Started_with_Jupyter_Locally.md). \n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Note, this notebook is compatible with Python 3.12 and well as Python 3.11, the default in Colab at the time of publishing this recipe. To check your python version, you can run the `!python --version` command in a code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Set up a watsonx.ai instance\n",
    "\n",
    "See [Getting Started with IBM watsonx](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_WatsonX.ipynb) for information on getting ready to use watsonx.ai. \n",
    "\n",
    "You will need three credentials from the watsonx.ai set up to add to your environment: `WATSONX_URL`, `WATSONX_APIKEY`, and `WATSONX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Install relevant libraries and set up credentials and the Granite model\n",
    "\n",
    "We'll need a few libraries for this recipe. We will be using LangGraph and LangChain libraries to use Granite on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/adityagidh/Documents/workspace/ai-log-triage/venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ibm-cos-sdk-core 2.14.3 requires urllib3<3,>=2.5.0, but you have urllib3 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/adityagidh/Documents/workspace/ai-log-triage/venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kubernetes 34.1.0 requires urllib3<2.4.0,>=1.24.2, but you have urllib3 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/adityagidh/Documents/workspace/ai-log-triage/venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install core libraries\n",
    "%pip install -qU langchain langchain-ibm langgraph langgraph-bigtool toolregistry\n",
    "\n",
    "# Install RAG components (Vector Store and utilities)\n",
    "%pip install -q chromadb langchain-chroma\n",
    "\n",
    "# Install IBM specific utility for easy credentials load\n",
    "%pip install -q ibm-watsonx-ai \"git+https://github.com/ibm-granite-community/utils.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Authentication and model initialization \n",
    "\n",
    "The next step involves initialization of the watsonx LLM (used for the agent's reasoning) and watsonx Embeddings (for Tool-RAG semantic search).\n",
    "\n",
    "**Note:** Ensure your environment variables (`WATSONX_URL`, `WATSONX_APIKEY`, `WATSONX_PROJECT_ID`) are set before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityagidh/Documents/workspace/ai-log-triage/venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3670: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your WATSONX_URL:  ········\n",
      "Please enter your WATSONX_APIKEY:  ········\n",
      "Please enter your WATSONX_PROJECT_ID:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: ibm/granite-3-3-8b-instruct\n",
      "Embeddings initialized: ibm/granite-embedding-107m-multilingual\n",
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from typing import List, Dict, Any\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from toolregistry import ToolRegistry\n",
    "\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain.chat_models import init_chat_model\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "\n",
    "# --- Configuration ---\n",
    "model = \"ibm/granite-3-3-8b-instruct\"\n",
    "\n",
    "llm_params = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "# --- 1. LLM Initialization (Agent's Brain) ---\n",
    "llm = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=llm_params,\n",
    ")\n",
    "print(f\"LLM initialized: {model}\")\n",
    "\n",
    "\n",
    "# --- 2. Embeddings Initialization (Tool-RAG Indexer) ---\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-107m-multilingual\",\n",
    "    url=get_env_var(\"WATSONX_URL\"),\n",
    "    apikey=get_env_var(\"WATSONX_APIKEY\"),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3\n",
    "    }\n",
    ")\n",
    "print(\"Embeddings initialized: ibm/granite-embedding-107m-multilingual\")\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose `ibm/granite-embedding-107m-multilingual` because it is a model from the [IBM Watsonx family](https://www.ibm.com/docs/en/watsonx/saas?topic=models-granite-embedding-107m-multilingual-model-card), ensuring seamless integration with other Watsonx components.\n",
    "\n",
    "Its multilingual capability is a benefit for real-world scenarios, and the '107m' size provides a good balance between performance and embedding quality for semantic retrieval of tool descriptions. We use ChromaDB as the underlying vector store for tool metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define small tools and data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ToolRAG concept, and the langgraph-bigtool library, are specifically designed to address the scalability problem that arises when an agent has hundreds or thousands of tools. In classic \"tool-calling\" architectures, the definitions of all tools must be inserted directly into the LLM's prompt, which quickly hits the context window limit and degrades performance.\n",
    "\n",
    "For this Proof of Concept (PoC), we use a small set of 5 tools for the following reasons:\n",
    "- Isolation of ToolRAG mechanics: By keeping the total tool count small, we can easily verify that the core ToolRAG retrieval mechanism is working correctly. When a query is given, we can visually confirm that the vector store is correctly selecting the top K=3 most relevant tool definitions (e.g., Finance tools for a financial query), even though the LLM could technically handle all 5 tools without RAG.\n",
    "- Focus on selection logic, and avoid context overflow: This setup allows us to focus on the agent's ability to select a filtered subset of tools (get_weather_forecast, get_stock_price, convert_currency) from the small indexed pool, and then correctly chain their execution, without introducing the complexity of context window management or large-scale tool embedding.\n",
    "- Simulating a scalable environment: In a production setting, this small set of 5 tools would ideally be hundreds of other tools (e.g., 50 financial, 50 IT, 50 HR tools). The retrieval mechanism shown here is what scales to those environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculate_future_value(principal: float, rate: float, years: int) -> str:\n",
    "    \"\"\"Calculates the future value of an investment using compound interest.\"\"\"\n",
    "    future_value = principal * ((1 + rate) ** years)\n",
    "    return f\"The future value is ${future_value:.2f}.\"\n",
    "\n",
    "@tool\n",
    "def get_stock_price(ticker: str) -> str:\n",
    "    \"\"\"Fetches the current or historical stock price for a given ticker symbol (e.g., IBM).\"\"\"\n",
    "    # Placeholder for a real API call\n",
    "    if ticker == \"IBM\":\n",
    "        return \"The current price for IBM is $185.50.\"\n",
    "    return f\"Stock price for {ticker} not found in this mock-up.\"\n",
    "\n",
    "@tool\n",
    "def check_developer_skill(skill: str) -> str:\n",
    "    \"\"\"Checks the availability of an AI developer with a specific programming or ML skill.\"\"\"\n",
    "    if 'python' in skill.lower() or 'langchain' in skill.lower():\n",
    "        return \"Yes, we have multiple experienced developers with that skill set.\"\n",
    "    return \"Developer with that specific skill is currently limited.\"\n",
    "\n",
    "@tool\n",
    "def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "    \"\"\"Converts a monetary amount between two specified currencies (e.g., USD to EUR).\"\"\"\n",
    "    # Placeholder for a real currency conversion\n",
    "    if from_currency == \"USD\" and to_currency == \"EUR\":\n",
    "        converted = amount * 0.92\n",
    "        return f\"{amount} USD is approximately {converted:.2f} EUR.\"\n",
    "    return f\"Conversion from {from_currency} to {to_currency} is not supported.\"\n",
    "\n",
    "@tool\n",
    "def get_weather_forecast(city: str) -> str:\n",
    "    \"\"\"Provides the current weather forecast for a specified city.\"\"\"\n",
    "    if 'boston' in city.lower():\n",
    "        return \"Boston's current weather is partly cloudy with a temperature of 15°C.\"\n",
    "    return f\"Weather data for {city} is not available in this mock-up.\"\n",
    "\n",
    "tools = [\n",
    "    calculate_future_value, \n",
    "    get_stock_price, \n",
    "    check_developer_skill, \n",
    "    convert_currency, \n",
    "    get_weather_forecast\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can also use `langgraph-bigtool` as your foundational step to define and build a tool set. Alternatively, you can wire these tools up with real web APIs to tailor the functions to be used as tools for the agent. Refer to the step 4 in the [Function_Calling_Agent.ipynb](https://github.com/ibm-granite-community/granite-agent-cookbook/blob/main/recipes/Function_Calling/Function_Calling_Agent.ipynb) for more details.\n",
    "\n",
    "The true power of `langgraph-bigtool` lies in its ability to support a very large toolset (100+ tools). By leveraging the `ToolRegistry` and the underlying vector store, you can onboard a massive library of domain-specific APIs and functions. The agent will then efficiently scout through this library, retrieving only the most pertinent tools for any user request, effectively enabling a single agent to be a master of many domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create tool registry and index tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the following steps here:\n",
    "\n",
    "- Initialize tool registry. This is the registry that holds the tools, but is now decoupled from the storage.\n",
    "- Register/index the tools into the Chroma vector store.\n",
    "- Get the documents/metadata from the tools\n",
    "- Add the tool documents to the vector store\n",
    "- Create the retriever for the BigTool agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tools registered in ToolRegistry: 5\n",
      "Tools indexed in Chroma vectorstore: 5\n"
     ]
    }
   ],
   "source": [
    "tool_registry = ToolRegistry()\n",
    "\n",
    "# Register tools individually using a loop\n",
    "tools_to_register = []\n",
    "for tool_obj in tools:\n",
    "    try:\n",
    "        tool_registry.register(\n",
    "            tool_or_func=tool_obj, \n",
    "            name=tool_obj.name,\n",
    "            description=tool_obj.description\n",
    "        )\n",
    "        tools_to_register.append(tool_obj)\n",
    "    except Exception as e:\n",
    "        print(f\"Error registering tool {tool_obj.name}: {e}\")\n",
    "        try:\n",
    "            tool_registry.register_from_langchain(tool_obj)\n",
    "            tools_to_register.append(tool_obj)\n",
    "        except Exception as e_inner:\n",
    "            print(f\"Error registering tool {tool_obj.name} with all methods: {e_inner}\")\n",
    "            \n",
    "# Get the documents/metadata from the tools\n",
    "tool_docs = []\n",
    "for tool_obj in tools_to_register:\n",
    "    if isinstance(tool_obj, BaseTool):\n",
    "        tool_docs.append(\n",
    "            Document(\n",
    "                page_content=tool_obj.description,\n",
    "                metadata={\"tool_name\": tool_obj.name}\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Add the tool documents to the vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=tool_docs,\n",
    "    embedding=watsonx_embedding\n",
    ")\n",
    "\n",
    "# Create the retriever for the BigTool agent\n",
    "tool_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 relevant tools\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "print(f\"Total tools registered in ToolRegistry: {len(tool_registry._tools)}\")\n",
    "print(f\"Tools indexed in Chroma vectorstore: {len(tool_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Agent capabilities: tool selection and execution\n",
    "\n",
    "The `langgraph-bigtool` libary abstracts the `ToolRAG` pattern into a streamlined `create_agent` function. \n",
    "\n",
    "The default retrieval function in langgraph-bigtool searches the vectorstore. In our example here,\n",
    "we will configure it to retrieve a maximum of 3 tools. The goal is to feature the core idea of vector store retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def custom_retrieve_tools(query: str, limit: int = 5) -> list[str]:\n",
    "    \"\"\"Retrieves the top N tool names most semantically similar to the query.\"\"\"\n",
    "    \n",
    "    retriever_instance = tool_registry._retriever\n",
    "\n",
    "    try:\n",
    "        vectorstore = tool_registry.vectorstore # Your current guess\n",
    "    except AttributeError:\n",
    "        # Fallback to an internal attribute if the public one is missing/renamed\n",
    "        vectorstore = tool_registry._vectorstore \n",
    "\n",
    "    # Execute the vector search\n",
    "    results = vectorstore.similarity_search_with_score(query, k=limit)\n",
    "    \n",
    "    tool_names = [result[0].metadata['tool_name'] for result in results]\n",
    "    \n",
    "    print(f\"\\n[ToolRAG Retrieval]: Found {len(tool_names)} relevant tools based on query.\")\n",
    "    return tool_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ToolRAG Agent\n",
    "\n",
    "The `create_agent` function handles the LangGraph state machine and RAG-tool fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolRag Agent compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "from langgraph_bigtool import create_agent\n",
    "from langgraph.store.memory import InMemoryStore as LangGraphStore\n",
    "\n",
    "tool_map = { \n",
    "    name: tool_registry.get_callable(name)\n",
    "    for name in tool_registry._tools.keys() \n",
    "}\n",
    "\n",
    "store = LangGraphStore()\n",
    "\n",
    "agent_builder = create_agent(\n",
    "    llm=llm,\n",
    "    tool_registry=tool_map,\n",
    "    retrieve_tools_function=custom_retrieve_tools\n",
    ")\n",
    "\n",
    "agent = agent_builder.compile(checkpointer=store) \n",
    "print(\"ToolRag Agent compiled successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the agent with a query requiring 3 tools or a subset of tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Agent with Query: What's the weather like in Boston, and can you check the stock price for IBM, then convert 100 USD to EUR? ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tool_rag_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Running Agent with Query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Run the agent\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m final_result = \u001b[43mtool_rag_agent\u001b[49m.invoke(\n\u001b[32m      7\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=user_query)]}\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Extract and print the final response\u001b[39;00m\n\u001b[32m     11\u001b[39m final_message = final_result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'tool_rag_agent' is not defined"
     ]
    }
   ],
   "source": [
    "user_query = \"What's the weather like in Boston, and can you check the stock price for IBM, then convert 100 USD to EUR?\"\n",
    "\n",
    "print(f\"--- Running Agent with Query: {user_query} ---\")\n",
    "\n",
    "# Run the agent\n",
    "final_result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=user_query)]}\n",
    ")\n",
    "\n",
    "# Extract and print the final response\n",
    "final_message = final_result[\"messages\"][-1]\n",
    "print(\"\\n--- Final Agent Response ---\")\n",
    "print(final_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-env)",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
