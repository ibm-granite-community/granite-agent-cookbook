{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Granite Function Calling Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this recipe, you will use the IBM® [Granite](https://www.ibm.com/granite) model now available on watsonx.ai™ to perform custom function calling.  \n",
    "\n",
    "Traditional [large language models (LLMs)](https://www.ibm.com/topics/large-language-models), like the OpenAI GPT-5 (generative pre-trained transformer) model available through ChatGPT, and the IBM Granite™ models that we'll use in this recipe, are limited in their knowledge and reasoning. They produce their responses based on the data used to train them and are difficult to adapt to personalized user queries. To obtain the missing information, these [generative AI](https://www.ibm.com/topics/generative-ai) models can integrate external tools within the function calling. This method is one way to avoid fine-tuning a foundation model for each specific use-case. The function calling examples in this recipe will implement external [API](https://www.ibm.com/topics/api) calls. \n",
    "\n",
    "The Granite model and tokenizer use [natural language processing (NLP)](https://www.ibm.com/topics/natural-language-processing) to parse query syntax. In addition, the models use function descriptions and function parameters to determine the appropriate tool calls. Key information is then extracted from user queries to be passed as function arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Before testing your agent, you'll need to set up the environment and create a basic function-calling agent. The following prerequisites walk you through the necessary setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Set up your environment\n",
    "\n",
    "While you can choose from several tools, this recipe is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations. \n",
    "\n",
    "You can run this notebook in [Colab](https://colab.research.google.com/drive/1kZl5o2oDJEQ72kLedaetQ7UXqFU9BEW2?usp=sharing), or download it to your system and [run the notebook locally](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started_with_Jupyter_Locally/Getting_Started_with_Jupyter_Locally.md). \n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Note, this notebook is compatible with Python 3.12 and well as Python 3.11, the default in Colab at the time of publishing this recipe. To check your python version, you can run the `!python --version` command in a code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Set up a watsonx.ai instance\n",
    "\n",
    "See [Getting Started with IBM watsonx](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_WatsonX.ipynb) for information on getting ready to use watsonx.ai. \n",
    "\n",
    "You will need three credentials from the watsonx.ai set up to add to your environment: `WATSONX_URL`, `WATSONX_APIKEY`, and `WATSONX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Install relevant libraries and set up credentials and the Granite model\n",
    "\n",
    "We'll need a few libraries for this recipe. We will be using LangGraph and LangChain libraries to use Granite on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / upgrade dependencies used in this notebook.\n",
    "# Use %pip (not !pip) so installs target the active Jupyter kernel environment.\n",
    "%pip install -q -U \"git+https://github.com/ibm-granite-community/utils.git\" \"langgraph>=0.2.0\" langgraph-prebuilt langchain langchain_ibm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now we will get the credentials to use watsonx.ai and create the Granite model for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = \"ibm/granite-4-h-small\"\n",
    "\n",
    "model_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "llm_granite = init_chat_model(\n",
    "    model=model,\n",
    "    model_provider=\"ibm\",\n",
    "    url=convert_to_secret_str(get_env_var(\"WATSONX_URL\")),\n",
    "    apikey=convert_to_secret_str(get_env_var(\"WATSONX_APIKEY\")),\n",
    "    project_id=get_env_var(\"WATSONX_PROJECT_ID\"),\n",
    "    params=model_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Define the tools\n",
    "\n",
    "We define two functions to be used as tools by our agent. These functions can use real web APIs if you obtain the necessary API keys. If you are unable to get the API keys, the tools below will respond with a fixed, predetermined value for demonstration purposes.\n",
    "\n",
    "- **`get_stock_price`**: Uses an `AV_STOCK_API_KEY` from [Alpha Vantage](https://www.alphavantage.co/support/#api-key)\n",
    "- **`get_current_weather`**: Uses a `WEATHER_API_KEY` from [OpenWeather](https://home.openweathermap.org/users/sign_up)\n",
    "\n",
    "**Store these private keys in a separate `.env` file in the same level of your directory as this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_STOCK_API_KEY = convert_to_secret_str(get_env_var(\"AV_STOCK_API_KEY\", \"unset\"))\n",
    "\n",
    "WEATHER_API_KEY = convert_to_secret_str(get_env_var(\"WEATHER_API_KEY\", \"unset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The function's docstring and type information are important for generating proper tool information, as this will be the basis of the tool description provided to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_stock_price(ticker: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the lowest and highest stock prices for a given ticker and date.\n",
    "\n",
    "    Args:\n",
    "        ticker: The stock ticker symbol, for example, \"IBM\".\n",
    "        date: The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the low and high stock prices on the given date.\n",
    "    \"\"\"\n",
    "    print(f\"Getting stock price for {ticker} on {date}\")\n",
    "\n",
    "    apikey = AV_STOCK_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"low\": \"245.4500\",\n",
    "            \"high\": \"249.0300\"\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={apikey}\"\n",
    "        stock_data = requests.get(stock_url)\n",
    "        data = stock_data.json()\n",
    "        stock_low = data[\"Time Series (Daily)\"][date][\"3. low\"]\n",
    "        stock_high = data[\"Time Series (Daily)\"][date][\"2. high\"]\n",
    "        return {\n",
    "            \"low\": stock_low,\n",
    "            \"high\": stock_high\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return {\n",
    "            \"low\": \"none\",\n",
    "            \"high\": \"none\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The `get_current_weather` function retrieves the real-time weather in a given location using the Current Weather Data API via [OpenWeather](https://openweathermap.org/api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location (default: San Francisco).\n",
    "\n",
    "    Args:\n",
    "        location: The name of the city for which to retrieve the weather information.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing weather information such as temperature in celsius, weather description, and humidity.\n",
    "    \"\"\"\n",
    "    print(f\"Getting current weather for {location}\")\n",
    "    apikey=WEATHER_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"description\": \"thunderstorms\",\n",
    "            \"temperature\": 25.3,\n",
    "            \"humidity\": 94\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # API request to fetch weather data\n",
    "        weather_url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={apikey}&units=metric\"\n",
    "        weather_data = requests.get(weather_url)\n",
    "        data = weather_data.json()\n",
    "        # Extracting relevant weather details\n",
    "        weather_description = data[\"weather\"][0][\"description\"]\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "\n",
    "        # Returning weather details\n",
    "        return {\n",
    "            \"description\": weather_description,\n",
    "            \"temperature\": temperature,\n",
    "            \"humidity\": humidity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return {\n",
    "            \"description\": \"none\",\n",
    "            \"temperature\": \"none\",\n",
    "            \"humidity\": \"none\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Create the agent\n",
    "\n",
    "LangChain provides a convenient method to create a function-calling agent. You just need to provide the model and the list of tools. For a detailed walkthrough of building agents from scratch with LangGraph, see the [Function Calling Agent recipe](../Function_Calling/Function_Calling_Agent.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We use `create_agent` from LangChain to quickly build our function-calling agent. Not specifying the `prompt` argument means the agent will use the default system prompt for tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Ensure prebuilt components are available for LangChain agents.\n",
    "%pip install -q -U langgraph-prebuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "tools = [get_stock_price, get_current_weather]\n",
    "\n",
    "agent: CompiledStateGraph = create_agent(\n",
    "    model=llm_granite,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Let's verify the agent works with a simple query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    \"\"\"Agent state that holds the conversation messages.\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def run_agent(graph: CompiledStateGraph, user_input: str):\n",
    "    \"\"\"Helper function to run the agent and display the conversation.\"\"\"\n",
    "    user_message = HumanMessage(user_input)\n",
    "    print(user_message.pretty_repr())\n",
    "    input_state = State(messages=[user_message])\n",
    "    for event in graph.stream(input_state):\n",
    "        for value in event.values():\n",
    "            print(value[\"messages\"][-1].pretty_repr())\n",
    "\n",
    "# Test the agent with a simple query\n",
    "run_agent(agent, \"What is the weather in Miami?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Testing Your Agent\n",
    "\n",
    "Now that we have a working function-calling agent, let's create a structured test framework to evaluate its behavior. \n",
    "\n",
    "Testing AI agents is analogous to Test-Driven Development (TDD) in traditional software engineering. Just as TDD provides confidence that your code works as expected, agent testing ensures your AI behaves reliably and consistently. This is a **necessity for productionizing your agent**—without proper testing, you cannot confidently deploy updates or compare model alternatives.\n",
    "\n",
    "**Why test your agent?**\n",
    "\n",
    "- **Catch regressions early** when you update prompts, tool schemas, or models\n",
    "- **Compare alternatives objectively** (e.g., model A vs. model B)\n",
    "- **Ship with confidence** because you know core use cases still pass\n",
    "- **Faster debugging** by reproducing issues with specific test cases\n",
    "- **Documentation** as your test cases become living examples of expected behavior\n",
    "\n",
    "In this notebook, we'll focus on testing **tool-calling agents** and implement:\n",
    "1. Evaluation helpers for tool-call trajectories and responses\n",
    "2. Structured test cases for single-turn and multi-turn interactions\n",
    "3. Summary metrics tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 1: Define data structures and evaluation helpers\n",
    "\n",
    "Before running tests, we define how to evaluate agent outputs. This follows TDD principles: **write your assertions first**.\n",
    "\n",
    "For tool-calling agents, we care about two types of evaluation:\n",
    "\n",
    "- **Trajectory evaluation**: Did the agent call the right tools with the right parameters? Use exact matching when mistakes are risky (e.g., tools that write or delete data).\n",
    "- **Response evaluation**: Did the agent's final response contain the expected content? For deterministic tests, substring matching is fast and easy to debug. For flexible responses, consider LLM-as-a-judge or semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List\n",
    "import time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    \"\"\"Represents a single tool call made by the agent.\"\"\"\n",
    "    tool_name: str\n",
    "    tool_parameters: Dict[str, Any]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentTestResult:\n",
    "    \"\"\"The output of an agent test run, including tool calls and metrics.\"\"\"\n",
    "    tool_calls: List[ToolCall] = field(default_factory=list)\n",
    "    final_response: str = \"\"\n",
    "    latency_ms: float = 0.0\n",
    "    prompt_tokens: int = 0\n",
    "    response_tokens: int = 0\n",
    "    total_tokens: int = 0\n",
    "\n",
    "\n",
    "def trajectory_match(actual: List[ToolCall], expected: List[Dict[str, Any]]) -> bool:\n",
    "    \"\"\"Check if actual tool calls exactly match expected.\n",
    "    \n",
    "    Use exact match when mistakes are risky (e.g., tools that write or delete data).\n",
    "    \"\"\"\n",
    "    actual_norm = [{\"tool_name\": c.tool_name, \"tool_parameters\": c.tool_parameters} for c in actual]\n",
    "    return actual_norm == expected\n",
    "\n",
    "\n",
    "def response_match(actual: str, expected_contains: str) -> bool:\n",
    "    \"\"\"Check if actual response contains the expected substring.\n",
    "    \n",
    "    For deterministic tests, a simple substring check is fast and easy to debug.\n",
    "    For flexible responses, consider LLM-as-a-judge or semantic similarity.\n",
    "    \"\"\"\n",
    "    return expected_contains.lower() in actual.lower()\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Simple heuristic for token count (for demo purposes).\"\"\"\n",
    "    return max(1, len(text) // 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Step 2: Create a test runner for the agent\n",
    "\n",
    "We wrap our LangGraph agent in a test runner that captures tool calls, responses, and metrics for evaluation. The runner streams through the agent execution, capturing:\n",
    "- Tool calls from AI messages (tool name and arguments)\n",
    "- Final text response (when no tool calls are requested)\n",
    "- Performance metrics (latency, token estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def run_agent_for_test(graph: CompiledStateGraph, user_input: str) -> AgentTestResult:\n",
    "    \"\"\"Run the agent and collect results for testing purposes.\"\"\"\n",
    "    start = time.time()\n",
    "    tool_calls_made: List[ToolCall] = []\n",
    "    final_response = \"\"\n",
    "    \n",
    "    user_message = HumanMessage(user_input)\n",
    "    input_state = State(messages=[user_message])\n",
    "    \n",
    "    # Stream through the agent execution\n",
    "    for event in graph.stream(input_state):\n",
    "        for value in event.values():\n",
    "            last_message = value[\"messages\"][-1]\n",
    "            \n",
    "            # Capture tool calls from AI messages\n",
    "            if isinstance(last_message, AIMessage):\n",
    "                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "                    for tc in last_message.tool_calls:\n",
    "                        tool_calls_made.append(ToolCall(\n",
    "                            tool_name=tc['name'],\n",
    "                            tool_parameters=tc['args']\n",
    "                        ))\n",
    "                # Capture final text response (when no tool calls)\n",
    "                if last_message.content and not last_message.tool_calls:\n",
    "                    final_response = last_message.content\n",
    "    \n",
    "    latency_ms = (time.time() - start) * 1000\n",
    "    prompt_tokens = estimate_tokens(user_input)\n",
    "    response_tokens = estimate_tokens(final_response)\n",
    "    \n",
    "    return AgentTestResult(\n",
    "        tool_calls=tool_calls_made,\n",
    "        final_response=final_response,\n",
    "        latency_ms=latency_ms,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        response_tokens=response_tokens,\n",
    "        total_tokens=prompt_tokens + response_tokens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Step 3: Define the test set\n",
    "\n",
    "Each test case includes:\n",
    "- **Input**: The user query to be processed\n",
    "- **Expected tool calls**: The tools and parameters the agent should use\n",
    "- **Expected response**: A substring the final response should contain\n",
    "\n",
    "We cover key scenarios for our weather and stock price tools. Note how we test both basic functionality and parameter variations to ensure the agent properly extracts and normalizes information from queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_TESTS = [\n",
    "    {\n",
    "        \"name\": \"weather_query\",\n",
    "        \"input\": \"What is the weather in Miami?\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"Miami\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"Miami\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stock_price_query\",\n",
    "        \"input\": \"What were the IBM stock prices on September 5, 2025?\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_stock_price\", \"tool_parameters\": {\"ticker\": \"IBM\", \"date\": \"2025-09-05\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"IBM\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"weather_different_city\",\n",
    "        \"input\": \"Tell me the current weather in New York\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"New York\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"New York\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stock_different_ticker\",\n",
    "        \"input\": \"Get me the stock price for AAPL on January 15, 2025\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_stock_price\", \"tool_parameters\": {\"ticker\": \"AAPL\", \"date\": \"2025-01-15\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"AAPL\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Step 4: Run single-turn tests\n",
    "\n",
    "Single-turn tests verify basic functionality for each tool. We run each test case through the agent and evaluate both:\n",
    "- **Trajectory**: Did the agent call the correct tool(s)?\n",
    "- **Response**: Did the final answer contain the expected information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_turn_tests(test_graph: CompiledStateGraph, tests: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Run all single-turn tests and collect results.\"\"\"\n",
    "    results = []\n",
    "    for test in tests:\n",
    "        print(f\"Running test: {test['name']}...\")\n",
    "        output = run_agent_for_test(test_graph, test[\"input\"])\n",
    "        \n",
    "        # Trajectory evaluation: exact match on tool name + parameters\n",
    "        traj_ok = trajectory_match(output.tool_calls, test[\"expected_tool_calls\"])\n",
    "        resp_ok = response_match(output.final_response, test[\"expected_response_contains\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": test[\"name\"],\n",
    "            \"trajectory_ok\": traj_ok,\n",
    "            \"response_ok\": resp_ok,\n",
    "            \"latency_ms\": round(output.latency_ms, 2),\n",
    "            \"prompt_tokens\": output.prompt_tokens,\n",
    "            \"response_tokens\": output.response_tokens,\n",
    "            \"total_tokens\": output.total_tokens,\n",
    "            \"tool_calls\": [(tc.tool_name, tc.tool_parameters) for tc in output.tool_calls],\n",
    "            \"final_response\": output.final_response[:200] + \"...\" if len(output.final_response) > 200 else output.final_response\n",
    "        })\n",
    "        print(f\"  ✓ Trajectory: {traj_ok}, Response: {resp_ok}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the tests\n",
    "test_results = run_single_turn_tests(agent, AGENT_TESTS)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Step 5: Define multi-turn tests\n",
    "\n",
    "Real conversations don't happen in isolation. Users ask follow-up questions, reference previous context, and switch topics mid-conversation. **Multi-turn tests** verify that the agent can handle sequential queries in a conversation context.\n",
    "\n",
    "These tests check:\n",
    "- **Tool type switching**: Can the agent transition between different tool types within a conversation?\n",
    "- **Contextual reference handling**: Does the agent understand follow-up questions like \"How about in Tokyo?\" without explicit mention of \"weather\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_TURN_TESTS = [\n",
    "    {\n",
    "        \"name\": \"weather_then_stock\",\n",
    "        \"turns\": [\n",
    "            {\n",
    "                \"input\": \"What is the weather in Boston?\",\n",
    "                \"expected_tool_calls\": [\n",
    "                    {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"Boston\"}}\n",
    "                ],\n",
    "                \"expected_response_contains\": \"Boston\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"Now tell me the IBM stock price on January 10, 2025\",\n",
    "                \"expected_tool_calls\": [\n",
    "                    {\"tool_name\": \"get_stock_price\", \"tool_parameters\": {\"ticker\": \"IBM\", \"date\": \"2025-01-10\"}}\n",
    "                ],\n",
    "                \"expected_response_contains\": \"IBM\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"multiple_weather_queries\",\n",
    "        \"turns\": [\n",
    "            {\n",
    "                \"input\": \"What's the weather like in London?\",\n",
    "                \"expected_tool_calls\": [\n",
    "                    {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"London\"}}\n",
    "                ],\n",
    "                \"expected_response_contains\": \"London\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"How about in Tokyo?\",\n",
    "                \"expected_tool_calls\": [\n",
    "                    {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"Tokyo\"}}\n",
    "                ],\n",
    "                \"expected_response_contains\": \"Tokyo\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"And what about Paris?\",\n",
    "                \"expected_tool_calls\": [\n",
    "                    {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"Paris\"}}\n",
    "                ],\n",
    "                \"expected_response_contains\": \"Paris\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Step 6: Run multi-turn tests\n",
    "\n",
    "For multi-turn tests, we run each turn through the agent and verify that the correct tool was invoked. Note that LangGraph maintains conversation history in the state automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_turn_tests(test_graph: CompiledStateGraph, tests: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Run multi-turn tests where each test has multiple conversation turns.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for test in tests:\n",
    "        print(f\"\\nRunning multi-turn test: {test['name']}\")\n",
    "        turn_results = []\n",
    "        \n",
    "        for i, turn in enumerate(test[\"turns\"]):\n",
    "            print(f\"  Turn {i+1}: {turn['input'][:50]}...\")\n",
    "            output = run_agent_for_test(test_graph, turn[\"input\"])\n",
    "            \n",
    "            # Trajectory evaluation: prefer exact matching on tool calls (name + params)\n",
    "            if \"expected_tool_calls\" in turn:\n",
    "                traj_ok = trajectory_match(output.tool_calls, turn[\"expected_tool_calls\"])\n",
    "            else:\n",
    "                # Backwards-compatibility with older schema (tool name only)\n",
    "                expected_name = turn.get(\"expected_tool_name\")\n",
    "                traj_ok = bool(expected_name) and any(tc.tool_name == expected_name for tc in output.tool_calls)\n",
    "            \n",
    "            resp_ok = response_match(output.final_response, turn[\"expected_response_contains\"])\n",
    "            \n",
    "            turn_results.append({\n",
    "                \"input\": turn[\"input\"],\n",
    "                \"trajectory_ok\": traj_ok,\n",
    "                \"response_ok\": resp_ok,\n",
    "                \"tool_calls\": [(tc.tool_name, tc.tool_parameters) for tc in output.tool_calls],\n",
    "                \"final_response\": output.final_response[:100] + \"...\" if len(output.final_response) > 100 else output.final_response,\n",
    "            })\n",
    "            print(f\"    ✓ Trajectory: {traj_ok}, Response: {resp_ok}\")\n",
    "        \n",
    "        all_results.append({\"name\": test[\"name\"], \"turns\": turn_results})\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run multi-turn tests\n",
    "multi_turn_results = run_multi_turn_tests(agent, MULTI_TURN_TESTS)\n",
    "multi_turn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Step 7: Compute summary metrics\n",
    "\n",
    "Summary metrics provide a high-level view of agent performance. Key metrics to track include:\n",
    "\n",
    "- **Pass Rate**: Percentage of tests meeting all success criteria—the primary indicator of correctness\n",
    "- **Average Latency**: Directly impacts user experience; watch for regressions after model updates\n",
    "- **Average Tokens**: Correlates with operational costs; evaluate if quality improvements justify increased cost\n",
    "\n",
    "These metrics should be tracked over time to enable early detection of regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-turn summary\n",
    "passed = sum(1 for r in test_results if r[\"trajectory_ok\"] and r[\"response_ok\"])\n",
    "total = len(test_results)\n",
    "avg_latency = round(sum(r[\"latency_ms\"] for r in test_results) / total, 2) if total > 0 else 0\n",
    "avg_total_tokens = round(sum(r[\"total_tokens\"] for r in test_results) / total, 2) if total > 0 else 0\n",
    "\n",
    "single_turn_summary = {\n",
    "    \"passed\": passed,\n",
    "    \"total\": total,\n",
    "    \"pass_rate\": f\"{(passed/total)*100:.1f}%\" if total > 0 else \"N/A\",\n",
    "    \"avg_latency_ms\": avg_latency,\n",
    "    \"avg_total_tokens\": avg_total_tokens\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SINGLE-TURN TEST SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Tests Passed: {passed}/{total}\")\n",
    "print(f\"Pass Rate: {single_turn_summary['pass_rate']}\")\n",
    "print(f\"Average Latency: {avg_latency} ms\")\n",
    "print(f\"Average Tokens: {avg_total_tokens}\")\n",
    "print()\n",
    "\n",
    "# Multi-turn summary\n",
    "multi_turn_passed = 0\n",
    "multi_turn_total = 0\n",
    "\n",
    "for test in multi_turn_results:\n",
    "    for turn in test[\"turns\"]:\n",
    "        multi_turn_total += 1\n",
    "        if turn[\"trajectory_ok\"] and turn[\"response_ok\"]:\n",
    "            multi_turn_passed += 1\n",
    "\n",
    "multi_turn_summary = {\n",
    "    \"passed\": multi_turn_passed,\n",
    "    \"total\": multi_turn_total,\n",
    "    \"pass_rate\": f\"{(multi_turn_passed/multi_turn_total)*100:.1f}%\" if multi_turn_total > 0 else \"N/A\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MULTI-TURN TEST SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Turns Passed: {multi_turn_passed}/{multi_turn_total}\")\n",
    "print(f\"Pass Rate: {multi_turn_summary['pass_rate']}\")\n",
    "print()\n",
    "\n",
    "# Overall summary\n",
    "overall_passed = passed + multi_turn_passed\n",
    "overall_total = total + multi_turn_total\n",
    "print(\"=\" * 50)\n",
    "print(\"OVERALL TEST SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Passed: {overall_passed}/{overall_total}\")\n",
    "print(f\"Overall Pass Rate: {(overall_passed/overall_total)*100:.1f}%\" if overall_total > 0 else \"N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this recipe, you learned how to build a structured testing framework for function-calling agents:\n",
    "\n",
    "1. **Defined evaluation helpers** for trajectory (tool calls) and response validation\n",
    "2. **Created a test runner** that captures tool calls, responses, and performance metrics\n",
    "3. **Wrote structured test cases** covering single-turn and multi-turn interactions\n",
    "4. **Computed summary metrics** including pass rates, latency, and token usage\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- **Expand your test set** with edge cases and failure scenarios (e.g., invalid inputs, non-existent cities)\n",
    "- **Add more tools** to test complex multi-tool interactions\n",
    "- **Implement LLM-as-a-judge** for response evaluation when substring matching isn't sufficient\n",
    "- **Integrate with CI/CD** to run tests automatically on every code change\n",
    "- **Compare model performance** by running the same tests with different models\n",
    "- **Track metrics over time** by storing results in a database and building dashboards\n",
    "\n",
    "For more on agent evaluation concepts and approaches, see the companion guide: [Test-Driven Agent Development](../../testing_agents.md)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
