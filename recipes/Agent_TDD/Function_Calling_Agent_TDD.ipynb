{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Granite Function Calling Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this recipe, you will use the IBM® [Granite](https://www.ibm.com/granite) model now available on watsonx.ai™ to perform custom function calling.  \n",
    "\n",
    "Traditional [large language models (LLMs)](https://www.ibm.com/topics/large-language-models), like the OpenAI GPT-5 (generative pre-trained transformer) model available through ChatGPT, and the IBM Granite™ models that we'll use in this recipe, are limited in their knowledge and reasoning. They produce their responses based on the data used to train them and are difficult to adapt to personalized user queries. To obtain the missing information, these [generative AI](https://www.ibm.com/topics/generative-ai) models can integrate external tools within the function calling. This method is one way to avoid fine-tuning a foundation model for each specific use-case. The function calling examples in this recipe will implement external [API](https://www.ibm.com/topics/api) calls. \n",
    "\n",
    "The Granite model and tokenizer use [natural language processing (NLP)](https://www.ibm.com/topics/natural-language-processing) to parse query syntax. In addition, the models use function descriptions and function parameters to determine the appropriate tool calls. Key information is then extracted from user queries to be passed as function arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this recipe is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations. \n",
    "\n",
    "You can run this notebook in [Colab](https://colab.research.google.com/), or download it to your system and [run the notebook locally](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started_with_Jupyter_Locally/Getting_Started_with_Jupyter_Locally.md). \n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Note, this notebook is compatible with Python 3.12 and well as Python 3.11, the default in Colab at the time of publishing this recipe. To check your python version, you can run the `!python --version` command in a code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2. Set up a watsonx.ai instance\n",
    "\n",
    "See [Getting Started with IBM watsonx](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_WatsonX.ipynb) for information on getting ready to use watsonx.ai. \n",
    "\n",
    "You will need three credentials from the watsonx.ai set up to add to your environment: `WATSONX_URL`, `WATSONX_APIKEY`, and `WATSONX_PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Step 3. Install relevant libraries and set up credentials and the Granite model\n",
    "\n",
    "We'll need a few libraries for this recipe. We will be using LangGraph and LangChain libraries to use Granite on watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::group::Install Dependencies\n",
      "Requirement already satisfied: uv in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (0.9.28)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2mUsing Python 3.12.4 environment at: /Users/aakritiaggarwal/Documents/cookbook/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m49 packages\u001b[0m \u001b[2min 2.55s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m49 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "::endgroup::\n"
     ]
    }
   ],
   "source": [
    "! echo \"::group::Install Dependencies\"\n",
    "%pip install uv\n",
    "! uv pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    langgraph \\\n",
    "    langchain \\\n",
    "    langchain_ibm\n",
    "! echo \"::endgroup::\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now we will get the credentials to use watsonx.ai and create the Granite model for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab079eb1",
   "metadata": {},
   "source": [
    "<!-- WATSONX_URL=https://eu-de.ml.cloud.ibm.com\n",
    "WATSONX_APIKEY=NN0oeSkAgUWb5SWrc7kXsqwnj2zkh3AgmX64K7cUiX32\n",
    "WATSONX_PROJECT_ID=f45e05e8-535a-4cc5-b51c-f7b3cf520943 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f652f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (1.2.7)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain) (1.2.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain) (1.0.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.6.7)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11.6)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aakritiaggarwal/Documents/cookbook/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain) (2.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "from langchain_core.utils.utils import convert_to_secret_str\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "model = \"ibm/granite-4-h-small\"\n",
    "\n",
    "model_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 200,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "# llm = init_chat_model(\n",
    "#     model=model,\n",
    "#     model_provider=\"ibm\",\n",
    "#     base_url=\"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/granite-4-8b/v1\",\n",
    "#     default_headers={\"RITS_API_KEY\": \"fb86f7e3c7d04ec083157e4358e6b37b\"},\n",
    "#     params=model_parameters,\n",
    "# )\n",
    "\n",
    "llm_granite = ChatOpenAI(\n",
    "    model_name=\"ibm-granite/granite-4.0-8b\",\n",
    "    api_key=\"not-needed\",\n",
    "    base_url=\"https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/granite-4-8b/v1\",\n",
    "    default_headers={\"RITS_API_KEY\": \"fb86f7e3c7d04ec083157e4358e6b37b\"},\n",
    "    temperature=0.0,  # matches what Concert agent uses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 4: Define the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We define two functions to be used as tools by our agent. These functions can use real web API if you obtain the necessary API keys. If you are unable to get the API keys, the tools below will respond with a fixed, predetermined value for demonstration purposes.\n",
    "\n",
    "The `get_stock_price` function in this recipe use an `AV_STOCK_API_KEY` key. To generate a free `AV_STOCK_API_KEY`, please visit the [Alpha Vantage website](https://www.alphavantage.co/support/#api-key). \n",
    "\n",
    "Secondly, the `get_current_weather` function uses a `WEATHER_API_KEY`. To generate one, please [create an account](https://home.openweathermap.org/users/sign_up). Upon creating an account, select the \"API Keys\" tab to display your free key.\n",
    "\n",
    "**Store these private keys in a separate `.env` file in the same level of your directory as this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_STOCK_API_KEY = convert_to_secret_str(get_env_var(\"AV_STOCK_API_KEY\", \"6GRX8LDPIPYLAP2X\"))\n",
    "\n",
    "WEATHER_API_KEY = convert_to_secret_str(get_env_var(\"WEATHER_API_KEY\", \"22f1103155f90f157576e342693b67ea\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We can now define our functions. The function's docstring and type information are important for generating the proper tool information since this information will be the basis of the tool description provided to the model.\n",
    "\n",
    "In this recipe, the `get_stock_price` function uses the Stock Market Data API available through Alpha Vantage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_stock_price(ticker: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the lowest and highest stock prices for a given ticker and date.\n",
    "\n",
    "    Args:\n",
    "        ticker: The stock ticker symbol, for example, \"IBM\".\n",
    "        date: The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the low and high stock prices on the given date.\n",
    "    \"\"\"\n",
    "    print(f\"Getting stock price for {ticker} on {date}\")\n",
    "\n",
    "    apikey = AV_STOCK_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"low\": \"245.4500\",\n",
    "            \"high\": \"249.0300\"\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={apikey}\"\n",
    "        stock_data = requests.get(stock_url)\n",
    "        data = stock_data.json()\n",
    "        stock_low = data[\"Time Series (Daily)\"][date][\"3. low\"]\n",
    "        stock_high = data[\"Time Series (Daily)\"][date][\"2. high\"]\n",
    "        return {\n",
    "            \"low\": stock_low,\n",
    "            \"high\": stock_high\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return {\n",
    "            \"low\": \"none\",\n",
    "            \"high\": \"none\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The `get_current_weather` function retrieves the real-time weather in a given location using the Current Weather Data API via [OpenWeather](https://openweathermap.org/api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location (default: San Francisco).\n",
    "\n",
    "    Args:\n",
    "        location: The name of the city for which to retrieve the weather information.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing weather information such as temperature in celsius, weather description, and humidity.\n",
    "    \"\"\"\n",
    "    print(f\"Getting current weather for {location}\")\n",
    "    apikey=WEATHER_API_KEY.get_secret_value()\n",
    "    if apikey == \"unset\":\n",
    "        print(\"No API key present; using a fixed, predetermined value for demonstration purposes\")\n",
    "        return {\n",
    "            \"description\": \"thunderstorms\",\n",
    "            \"temperature\": 25.3,\n",
    "            \"humidity\": 94\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        # API request to fetch weather data\n",
    "        weather_url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={apikey}&units=metric\"\n",
    "        weather_data = requests.get(weather_url)\n",
    "        data = weather_data.json()\n",
    "        # Extracting relevant weather details\n",
    "        weather_description = data[\"weather\"][0][\"description\"]\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "\n",
    "        # Returning weather details\n",
    "        return {\n",
    "            \"description\": weather_description,\n",
    "            \"temperature\": temperature,\n",
    "            \"humidity\": humidity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return {\n",
    "            \"description\": \"none\",\n",
    "            \"temperature\": \"none\",\n",
    "            \"humidity\": \"none\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Step 4: Build the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Now we use LangGraph to build the agent. First we setup the agent's state. The state is maintained by the agent as it handles a request. This state will hold the list of messages in the multi-turn conversation with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We now create a binding of the tools to the Granite model. When the model is called, the available tool descriptions are provided to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_stock_price, get_current_weather]\n",
    "llm_with_tools = llm_granite.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Now we begin to create the LangGraph graph for our agent. This graph consists of nodes which are functional blocks which are assembled into a graph with edges controlling workflow through the graph from a start node to an end node.\n",
    "\n",
    "The first node we create is for calling Granite. It uses the list of messages from the state as the input to the model and returns the response message from the model to update the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node(state: State) -> State:\n",
    "    messages = state[\"messages\"]\n",
    "    response_message = llm_with_tools.invoke(messages)\n",
    "    state_update = State(messages=[response_message])\n",
    "    return state_update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We also want a node for tool calling when the model requests a tool be called with some arguments. Here we use the pre-built tool calling node implementation from LangGraph.\n",
    "\n",
    "When the model wants to use a provided tool, it will request the tool name along with arguments. The tool node will call the tool with the arguments and add a tool message to the state with the results of the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "We also need some edges defined to control flow between the nodes we created.\n",
    "\n",
    "We define a function to examine the state and decide whether to call to the tool node if Granite requested to use a tool or proceed to the END node in the graph finishing the workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import END\n",
    "\n",
    "def route_tools(state: State) -> str:\n",
    "    \"\"\"\n",
    "    This is conditional_edge function to route to the ToolNode if the last message\n",
    "    in the state has tool calls. Otherwise, route to the END node to complete the\n",
    "    workflow.\n",
    "    \"\"\"\n",
    "    messages = state.get(\"messages\")\n",
    "    if not messages:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    # If the last message is from the model and it contains a tool call request\n",
    "    if isinstance(last_message, AIMessage) and len(last_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We now have defined some nodes and a function to act as an edge. \n",
    "\n",
    "Next we build the graph using the nodes and add edges between the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We add two nodes. One for our llm node and another for our tools node.\n",
    "\n",
    "The first argument is the unique node name. The second argument is the function or object that will be called whenever the node is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x126153bf0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node(\"llm\", llm_node)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "\n",
    "Then we add the initial edge from the `START` node to our llm node. This starts the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x126153bf0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START\n",
    "\n",
    "graph_builder.add_edge(START, \"llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Then we add a conditional edge from our llm node to either our tools node\n",
    "or the final `END` node. The `route_tools` function we define above returns `tools` if the llm asks to use a tool,\n",
    "and `END` if no tool call is requested.\n",
    "This conditional routing defines the main agent loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x126153bf0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "\n",
    "We then add an edge so that after a tool has been called, we return to our llm node to decide the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x126153bf0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_edge(\"tools\", \"llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Now that we have added all the nodes and edges to the builder, we compile this into our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "graph: CompiledStateGraph[State] = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "We can visualize the compiled graph to see the nodes and edges in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Step 5: Using the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "To use our agent, we define a method to process a request to the agent. The input is processed by the agent's graph and the event stream of messages processed by the agent is displayed so we can see the workflow of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def function_calling_agent(graph: CompiledStateGraph, user_input: str):\n",
    "    user_message = HumanMessage(user_input)\n",
    "    print(user_message.pretty_repr())\n",
    "    input = State(messages=[user_message])\n",
    "    for event in graph.stream(input):\n",
    "        for value in event.values():\n",
    "            print(value[\"messages\"][-1].pretty_repr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Let's ask some questions of the agent which rely upon using the provided tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Human Message =================================\n",
      "\n",
      "What is the weather in Miami?\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  get_current_weather (chatcmpl-tool-52998382f2554819beb8244d66e9f30e)\n",
      " Call ID: chatcmpl-tool-52998382f2554819beb8244d66e9f30e\n",
      "  Args:\n",
      "    location: Miami\n",
      "Getting current weather for Miami\n",
      "================================= Tool Message =================================\n",
      "Name: get_current_weather\n",
      "\n",
      "{\"description\": \"clear sky\", \"temperature\": 4.06, \"humidity\": 77}\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "The current weather in Miami is clear sky with a temperature of 4.06°C and humidity of 77%.\n"
     ]
    }
   ],
   "source": [
    "function_calling_agent(graph, \"What is the weather in Miami?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Human Message =================================\n",
      "\n",
      "What were the IBM stock prices on September 5, 2025?\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  get_stock_price (chatcmpl-tool-9fa90a8358404553b77d51b553115098)\n",
      " Call ID: chatcmpl-tool-9fa90a8358404553b77d51b553115098\n",
      "  Args:\n",
      "    ticker: IBM\n",
      "    date: 2025-09-05\n",
      "Getting stock price for IBM on 2025-09-05\n",
      "Error fetching stock data: '2025-09-05'\n",
      "================================= Tool Message =================================\n",
      "Name: get_stock_price\n",
      "\n",
      "{\"low\": \"none\", \"high\": \"none\"}\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "I'm sorry, but I couldn't find the stock prices for IBM on September 5, 2025. The data might not be available or there might be an issue with the request. Please try again later or check the date and ticker symbol for accuracy.\n"
     ]
    }
   ],
   "source": [
    "function_calling_agent(graph, \"What were the IBM stock prices on September 5, 2025?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## Step 6: Simplifying your agent creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "LangChain provides a method to that captures all of the work we did above to create a function calling agent. You just need to provide the model and the list of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Human Message =================================\n",
      "\n",
      "What is the weather in Miami?\n",
      "================================== Ai Message ==================================\n",
      "Tool Calls:\n",
      "  get_current_weather (chatcmpl-tool-0040652e8a4e42768f8f979867ac78d8)\n",
      " Call ID: chatcmpl-tool-0040652e8a4e42768f8f979867ac78d8\n",
      "  Args:\n",
      "    location: Miami\n",
      "Getting current weather for Miami\n",
      "================================= Tool Message =================================\n",
      "Name: get_current_weather\n",
      "\n",
      "{\"description\": \"clear sky\", \"temperature\": 4.06, \"humidity\": 77}\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "The current weather in Miami is clear sky with a temperature of 4.06°C and humidity of 77%.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm_granite,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "function_calling_agent(agent, \"What is the weather in Miami?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "A note about the `prompt` argument to `create_agent`. Not specifying the `prompt` argument means the agent will behave as a function calling agent that we previously built above. The `prompt` argument is used as a system prompt to the model. For Granite 3, providing a system prompt will mean the default system prompt for tool calling will be not be present and the model may not properly handle tool calling. So we do not specify the `prompt` argument here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a94e5",
   "metadata": {},
   "source": [
    "# Testing Your Agent\n",
    "\n",
    "Now that we have built a function-calling agent, let's create a structured test framework to evaluate its behavior. Testing agents is essential because:\n",
    "\n",
    "- **Catch regressions early** when you update prompts, tool schemas, or models.\n",
    "- **Compare alternatives objectively** (e.g., model A vs. model B).\n",
    "- **Ship with confidence** because you know core use cases still pass.\n",
    "\n",
    "We'll implement:\n",
    "1. Evaluation helpers for tool-call trajectories and responses\n",
    "2. Structured test cases\n",
    "3. Single-turn and multi-turn testing\n",
    "4. Summary metrics tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fda2e5",
   "metadata": {},
   "source": [
    "## Step 7: Define data structures and evaluation helpers\n",
    "\n",
    "Before running tests, we define how to evaluate agent outputs. This follows TDD principles: write your assertions first.\n",
    "\n",
    "We need two types of evaluation:\n",
    "- **Trajectory evaluation**: Did the agent call the right tools with the right parameters?\n",
    "- **Response evaluation**: Did the agent's final response contain the expected content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1933edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List\n",
    "import time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ToolCall:\n",
    "    \"\"\"Represents a single tool call made by the agent.\"\"\"\n",
    "    tool_name: str\n",
    "    tool_parameters: Dict[str, Any]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentTestResult:\n",
    "    \"\"\"The output of an agent test run, including tool calls and metrics.\"\"\"\n",
    "    tool_calls: List[ToolCall] = field(default_factory=list)\n",
    "    final_response: str = \"\"\n",
    "    latency_ms: float = 0.0\n",
    "    prompt_tokens: int = 0\n",
    "    response_tokens: int = 0\n",
    "    total_tokens: int = 0\n",
    "\n",
    "\n",
    "def trajectory_match(actual: List[ToolCall], expected: List[Dict[str, Any]]) -> bool:\n",
    "    \"\"\"Check if actual tool calls exactly match expected.\n",
    "    \n",
    "    Use exact match when mistakes are risky (e.g., tools that write or delete data).\n",
    "    \"\"\"\n",
    "    actual_norm = [{\"tool_name\": c.tool_name, \"tool_parameters\": c.tool_parameters} for c in actual]\n",
    "    return actual_norm == expected\n",
    "\n",
    "\n",
    "def response_match(actual: str, expected_contains: str) -> bool:\n",
    "    \"\"\"Check if actual response contains the expected substring.\n",
    "    \n",
    "    For deterministic tests, a simple substring check is fast and easy to debug.\n",
    "    For flexible responses, consider LLM-as-a-judge or semantic similarity.\n",
    "    \"\"\"\n",
    "    return expected_contains.lower() in actual.lower()\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Simple heuristic for token count (for demo purposes).\"\"\"\n",
    "    return max(1, len(text) // 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0b237",
   "metadata": {},
   "source": [
    "## Step 8: Create a test runner for the agent\n",
    "\n",
    "We wrap our LangGraph agent in a test runner that captures tool calls, responses, and metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8bb4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "def run_agent_for_test(graph: CompiledStateGraph, user_input: str) -> AgentTestResult:\n",
    "    \"\"\"Run the agent and collect results for testing purposes.\"\"\"\n",
    "    start = time.time()\n",
    "    tool_calls_made: List[ToolCall] = []\n",
    "    final_response = \"\"\n",
    "    \n",
    "    user_message = HumanMessage(user_input)\n",
    "    input_state = State(messages=[user_message])\n",
    "    \n",
    "    # Stream through the agent execution\n",
    "    for event in graph.stream(input_state):\n",
    "        for value in event.values():\n",
    "            last_message = value[\"messages\"][-1]\n",
    "            \n",
    "            # Capture tool calls from AI messages\n",
    "            if isinstance(last_message, AIMessage):\n",
    "                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "                    for tc in last_message.tool_calls:\n",
    "                        tool_calls_made.append(ToolCall(\n",
    "                            tool_name=tc['name'],\n",
    "                            tool_parameters=tc['args']\n",
    "                        ))\n",
    "                # Capture final text response (when no tool calls)\n",
    "                if last_message.content and not last_message.tool_calls:\n",
    "                    final_response = last_message.content\n",
    "    \n",
    "    latency_ms = (time.time() - start) * 1000\n",
    "    prompt_tokens = estimate_tokens(user_input)\n",
    "    response_tokens = estimate_tokens(final_response)\n",
    "    \n",
    "    return AgentTestResult(\n",
    "        tool_calls=tool_calls_made,\n",
    "        final_response=final_response,\n",
    "        latency_ms=latency_ms,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        response_tokens=response_tokens,\n",
    "        total_tokens=prompt_tokens + response_tokens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5956705f",
   "metadata": {},
   "source": [
    "## Step 9: Define the test set\n",
    "\n",
    "Each test case includes:\n",
    "- **Input**: The user query.\n",
    "- **Expected tool calls**: The tools and parameters the agent should use.\n",
    "- **Expected response**: A substring the final response should contain.\n",
    "\n",
    "We cover key scenarios for our weather and stock price tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf328fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_TESTS = [\n",
    "    {\n",
    "        \"name\": \"weather_query\",\n",
    "        \"input\": \"What is the weather in Miami?\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"Miami\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"Miami\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stock_price_query\",\n",
    "        \"input\": \"What were the IBM stock prices on September 5, 2025?\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_stock_price\", \"tool_parameters\": {\"ticker\": \"IBM\", \"date\": \"2025-09-05\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"IBM\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"weather_different_city\",\n",
    "        \"input\": \"Tell me the current weather in New York\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_current_weather\", \"tool_parameters\": {\"location\": \"New York\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"New York\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stock_different_ticker\",\n",
    "        \"input\": \"Get me the stock price for AAPL on January 15, 2025\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"tool_name\": \"get_stock_price\", \"tool_parameters\": {\"ticker\": \"AAPL\", \"date\": \"2025-01-15\"}}\n",
    "        ],\n",
    "        \"expected_response_contains\": \"AAPL\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd759cc1",
   "metadata": {},
   "source": [
    "## Step 10: Run single-turn tests\n",
    "\n",
    "We run each test case through the agent and evaluate both the trajectory (tool calls) and the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de43083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: weather_query...\n",
      "Getting current weather for Miami\n",
      "  ✓ Trajectory: True, Response: True\n",
      "Running test: stock_price_query...\n",
      "Getting stock price for IBM on 2025-09-05\n",
      "Error fetching stock data: '2025-09-05'\n",
      "  ✓ Trajectory: True, Response: True\n",
      "Running test: weather_different_city...\n",
      "Getting current weather for New York\n",
      "  ✓ Trajectory: True, Response: True\n",
      "Running test: stock_different_ticker...\n",
      "Getting stock price for AAPL on 2025-01-15\n",
      "Error fetching stock data: '2025-01-15'\n",
      "  ✓ Trajectory: True, Response: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'weather_query',\n",
       "  'trajectory_ok': True,\n",
       "  'response_ok': True,\n",
       "  'latency_ms': 1680.27,\n",
       "  'prompt_tokens': 7,\n",
       "  'response_tokens': 22,\n",
       "  'total_tokens': 29,\n",
       "  'tool_calls': [('get_current_weather', {'location': 'Miami'})],\n",
       "  'final_response': 'The current weather in Miami is clear sky with a temperature of 4.06°C and humidity of 77%.'},\n",
       " {'name': 'stock_price_query',\n",
       "  'trajectory_ok': True,\n",
       "  'response_ok': True,\n",
       "  'latency_ms': 2151.93,\n",
       "  'prompt_tokens': 13,\n",
       "  'response_tokens': 56,\n",
       "  'total_tokens': 69,\n",
       "  'tool_calls': [('get_stock_price', {'ticker': 'IBM', 'date': '2025-09-05'})],\n",
       "  'final_response': \"I'm sorry, but I couldn't find the stock prices for IBM on September 5, 2025. The data might not be available or there might be an issue with the request. Please try again later or check the date and ...\"},\n",
       " {'name': 'weather_different_city',\n",
       "  'trajectory_ok': True,\n",
       "  'response_ok': True,\n",
       "  'latency_ms': 1713.71,\n",
       "  'prompt_tokens': 9,\n",
       "  'response_tokens': 23,\n",
       "  'total_tokens': 32,\n",
       "  'tool_calls': [('get_current_weather', {'location': 'New York'})],\n",
       "  'final_response': 'The current weather in New York is clear sky with a temperature of -9.25°C and humidity of 55%.'},\n",
       " {'name': 'stock_different_ticker',\n",
       "  'trajectory_ok': True,\n",
       "  'response_ok': True,\n",
       "  'latency_ms': 2177.04,\n",
       "  'prompt_tokens': 12,\n",
       "  'response_tokens': 37,\n",
       "  'total_tokens': 49,\n",
       "  'tool_calls': [('get_stock_price',\n",
       "    {'ticker': 'AAPL', 'date': '2025-01-15'})],\n",
       "  'final_response': \"I'm sorry, but I couldn't find the stock prices for AAPL on January 15, 2025. The data returned was incomplete. Please check the date and try again.\"}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_single_turn_tests(test_graph: CompiledStateGraph, tests: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Run all single-turn tests and collect results.\"\"\"\n",
    "    results = []\n",
    "    for test in tests:\n",
    "        print(f\"Running test: {test['name']}...\")\n",
    "        output = run_agent_for_test(test_graph, test[\"input\"])\n",
    "        \n",
    "        # For trajectory matching, we check if the expected tool was called\n",
    "        # Note: LLM may include slight variations in parameters\n",
    "        traj_ok = len(output.tool_calls) == len(test[\"expected_tool_calls\"])\n",
    "        if traj_ok and len(output.tool_calls) > 0:\n",
    "            # Check tool names match\n",
    "            for actual, expected in zip(output.tool_calls, test[\"expected_tool_calls\"]):\n",
    "                if actual.tool_name != expected[\"tool_name\"]:\n",
    "                    traj_ok = False\n",
    "                    break\n",
    "        \n",
    "        resp_ok = response_match(output.final_response, test[\"expected_response_contains\"])\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": test[\"name\"],\n",
    "            \"trajectory_ok\": traj_ok,\n",
    "            \"response_ok\": resp_ok,\n",
    "            \"latency_ms\": round(output.latency_ms, 2),\n",
    "            \"prompt_tokens\": output.prompt_tokens,\n",
    "            \"response_tokens\": output.response_tokens,\n",
    "            \"total_tokens\": output.total_tokens,\n",
    "            \"tool_calls\": [(tc.tool_name, tc.tool_parameters) for tc in output.tool_calls],\n",
    "            \"final_response\": output.final_response[:200] + \"...\" if len(output.final_response) > 200 else output.final_response\n",
    "        })\n",
    "        print(f\"  ✓ Trajectory: {traj_ok}, Response: {resp_ok}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the tests\n",
    "test_results = run_single_turn_tests(graph, AGENT_TESTS)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24115323",
   "metadata": {},
   "source": [
    "## Step 11: Define multi-turn tests\n",
    "\n",
    "Multi-turn tests verify that the agent can handle sequential queries in a conversation context. For our tools, this might involve asking about weather in one city, then another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb3834ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_TURN_TESTS = [\n",
    "    {\n",
    "        \"name\": \"weather_then_stock\",\n",
    "        \"turns\": [\n",
    "            {\n",
    "                \"input\": \"What is the weather in Boston?\",\n",
    "                \"expected_tool_name\": \"get_current_weather\",\n",
    "                \"expected_response_contains\": \"Boston\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"Now tell me the IBM stock price on January 10, 2025\",\n",
    "                \"expected_tool_name\": \"get_stock_price\",\n",
    "                \"expected_response_contains\": \"IBM\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"multiple_weather_queries\",\n",
    "        \"turns\": [\n",
    "            {\n",
    "                \"input\": \"What's the weather like in London?\",\n",
    "                \"expected_tool_name\": \"get_current_weather\",\n",
    "                \"expected_response_contains\": \"London\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"How about in Tokyo?\",\n",
    "                \"expected_tool_name\": \"get_current_weather\",\n",
    "                \"expected_response_contains\": \"Tokyo\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"And what about Paris?\",\n",
    "                \"expected_tool_name\": \"get_current_weather\",\n",
    "                \"expected_response_contains\": \"Paris\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20043ff5",
   "metadata": {},
   "source": [
    "## Step 12: Run multi-turn tests\n",
    "\n",
    "For multi-turn tests, we maintain conversation state across turns by accumulating messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a660f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running multi-turn test: weather_then_stock\n",
      "  Turn 1: What is the weather in Boston?...\n",
      "Getting current weather for Boston\n",
      "    ✓ Tool: True, Response: True\n",
      "  Turn 2: Now tell me the IBM stock price on January 10, 202...\n",
      "Getting stock price for IBM on 2025-01-10\n",
      "Error fetching stock data: '2025-01-10'\n",
      "    ✓ Tool: True, Response: True\n",
      "\n",
      "Running multi-turn test: multiple_weather_queries\n",
      "  Turn 1: What's the weather like in London?...\n",
      "Getting current weather for London\n",
      "    ✓ Tool: True, Response: True\n",
      "  Turn 2: How about in Tokyo?...\n",
      "Getting current weather for Tokyo\n",
      "    ✓ Tool: True, Response: True\n",
      "  Turn 3: And what about Paris?...\n",
      "Getting current weather for Paris\n",
      "    ✓ Tool: True, Response: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'weather_then_stock',\n",
       "  'turns': [{'input': 'What is the weather in Boston?',\n",
       "    'tool_ok': True,\n",
       "    'response_ok': True,\n",
       "    'tool_calls': [('get_current_weather', {'location': 'Boston'})],\n",
       "    'final_response': 'The current weather in Boston is clear sky with a temperature of -8.82°C and humidity of 60%.'},\n",
       "   {'input': 'Now tell me the IBM stock price on January 10, 2025',\n",
       "    'tool_ok': True,\n",
       "    'response_ok': True,\n",
       "    'tool_calls': [('get_stock_price',\n",
       "      {'ticker': 'IBM', 'date': '2025-01-10'})],\n",
       "    'final_response': \"I'm sorry, but I couldn't find the stock price for IBM on January 10, 2025. The data returned indica...\"}]},\n",
       " {'name': 'multiple_weather_queries',\n",
       "  'turns': [{'input': \"What's the weather like in London?\",\n",
       "    'tool_ok': True,\n",
       "    'response_ok': True,\n",
       "    'tool_calls': [('get_current_weather', {'location': 'London'})],\n",
       "    'final_response': 'The current weather in London is light rain with a temperature of 7.23°C and a humidity level of 92%...'},\n",
       "   {'input': 'How about in Tokyo?',\n",
       "    'tool_ok': True,\n",
       "    'response_ok': True,\n",
       "    'tool_calls': [('get_current_weather', {'location': 'Tokyo'})],\n",
       "    'final_response': 'The current weather in Tokyo is partly cloudy with a temperature of 9.84°C and a humidity level of 3...'},\n",
       "   {'input': 'And what about Paris?',\n",
       "    'tool_ok': True,\n",
       "    'response_ok': True,\n",
       "    'tool_calls': [('get_current_weather', {'location': 'Paris'})],\n",
       "    'final_response': 'The current weather in Paris is foggy with a temperature of 2.54°C and a humidity level of 97%.'}]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_multi_turn_tests(test_graph: CompiledStateGraph, tests: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Run multi-turn tests where each test has multiple conversation turns.\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for test in tests:\n",
    "        print(f\"\\nRunning multi-turn test: {test['name']}\")\n",
    "        turn_results = []\n",
    "        \n",
    "        for i, turn in enumerate(test[\"turns\"]):\n",
    "            print(f\"  Turn {i+1}: {turn['input'][:50]}...\")\n",
    "            output = run_agent_for_test(test_graph, turn[\"input\"])\n",
    "            \n",
    "            # Check if the expected tool was called\n",
    "            tool_ok = any(tc.tool_name == turn[\"expected_tool_name\"] for tc in output.tool_calls)\n",
    "            resp_ok = response_match(output.final_response, turn[\"expected_response_contains\"])\n",
    "            \n",
    "            turn_results.append({\n",
    "                \"input\": turn[\"input\"],\n",
    "                \"tool_ok\": tool_ok,\n",
    "                \"response_ok\": resp_ok,\n",
    "                \"tool_calls\": [(tc.tool_name, tc.tool_parameters) for tc in output.tool_calls],\n",
    "                \"final_response\": output.final_response[:100] + \"...\" if len(output.final_response) > 100 else output.final_response\n",
    "            })\n",
    "            print(f\"    ✓ Tool: {tool_ok}, Response: {resp_ok}\")\n",
    "        \n",
    "        all_results.append({\"name\": test[\"name\"], \"turns\": turn_results})\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run multi-turn tests\n",
    "multi_turn_results = run_multi_turn_tests(graph, MULTI_TURN_TESTS)\n",
    "multi_turn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac723023",
   "metadata": {},
   "source": [
    "## Step 13: Compute summary metrics\n",
    "\n",
    "Aggregate results to get an overall view of agent performance across all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd73b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SINGLE-TURN TEST SUMMARY\n",
      "==================================================\n",
      "Tests Passed: 4/4\n",
      "Pass Rate: 100.0%\n",
      "Average Latency: 1930.74 ms\n",
      "Average Tokens: 44.75\n",
      "\n",
      "==================================================\n",
      "MULTI-TURN TEST SUMMARY\n",
      "==================================================\n",
      "Turns Passed: 5/5\n",
      "Pass Rate: 100.0%\n",
      "\n",
      "==================================================\n",
      "OVERALL TEST SUMMARY\n",
      "==================================================\n",
      "Total Passed: 9/9\n",
      "Overall Pass Rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Single-turn summary\n",
    "passed = sum(1 for r in test_results if r[\"trajectory_ok\"] and r[\"response_ok\"])\n",
    "total = len(test_results)\n",
    "avg_latency = round(sum(r[\"latency_ms\"] for r in test_results) / total, 2) if total > 0 else 0\n",
    "avg_total_tokens = round(sum(r[\"total_tokens\"] for r in test_results) / total, 2) if total > 0 else 0\n",
    "\n",
    "single_turn_summary = {\n",
    "    \"passed\": passed,\n",
    "    \"total\": total,\n",
    "    \"pass_rate\": f\"{(passed/total)*100:.1f}%\" if total > 0 else \"N/A\",\n",
    "    \"avg_latency_ms\": avg_latency,\n",
    "    \"avg_total_tokens\": avg_total_tokens\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SINGLE-TURN TEST SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Tests Passed: {passed}/{total}\")\n",
    "print(f\"Pass Rate: {single_turn_summary['pass_rate']}\")\n",
    "print(f\"Average Latency: {avg_latency} ms\")\n",
    "print(f\"Average Tokens: {avg_total_tokens}\")\n",
    "print()\n",
    "\n",
    "# Multi-turn summary\n",
    "multi_turn_passed = 0\n",
    "multi_turn_total = 0\n",
    "\n",
    "for test in multi_turn_results:\n",
    "    for turn in test[\"turns\"]:\n",
    "        multi_turn_total += 1\n",
    "        if turn[\"tool_ok\"] and turn[\"response_ok\"]:\n",
    "            multi_turn_passed += 1\n",
    "\n",
    "multi_turn_summary = {\n",
    "    \"passed\": multi_turn_passed,\n",
    "    \"total\": multi_turn_total,\n",
    "    \"pass_rate\": f\"{(multi_turn_passed/multi_turn_total)*100:.1f}%\" if multi_turn_total > 0 else \"N/A\"\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MULTI-TURN TEST SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Turns Passed: {multi_turn_passed}/{multi_turn_total}\")\n",
    "print(f\"Pass Rate: {multi_turn_summary['pass_rate']}\")\n",
    "print()\n",
    "\n",
    "# Overall summary\n",
    "overall_passed = passed + multi_turn_passed\n",
    "overall_total = total + multi_turn_total\n",
    "print(\"=\" * 50)\n",
    "print(\"OVERALL TEST SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Passed: {overall_passed}/{overall_total}\")\n",
    "print(f\"Overall Pass Rate: {(overall_passed/overall_total)*100:.1f}%\" if overall_total > 0 else \"N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbdb4b8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this recipe, you learned how to:\n",
    "\n",
    "1. **Build a function-calling agent** using LangGraph with Granite on watsonx.ai.\n",
    "2. **Define real tools** (weather and stock price APIs) for the agent to use.\n",
    "3. **Create a test framework** with data structures for tool calls and evaluation helpers.\n",
    "4. **Write structured test cases** for single-turn and multi-turn conversations.\n",
    "5. **Run tests and collect metrics** including pass rates, latency, and token usage.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- **Expand your test set** with more edge cases and failure scenarios.\n",
    "- **Add more tools** to test complex multi-tool interactions.\n",
    "- **Implement LLM-as-a-judge** for response evaluation when substring matching isn't enough.\n",
    "- **Integrate with CI/CD** to run tests automatically on every code change.\n",
    "- **Compare model performance** by running the same tests with different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
